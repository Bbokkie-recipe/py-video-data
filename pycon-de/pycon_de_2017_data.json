[
    {
        "index": 1,
        "title": "PyCon.DE 2017 Keynote Prof. Dr. Susanne Mertens - Neutrinos: who are you and if yes how many?",
        "url": "https://www.youtube.com/watch?v=3h-6GBBF4Hg",
        "video_id": "3h-6GBBF4Hg",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171127",
        "duration": 3442,
        "view_count": 2664,
        "like_count": 30,
        "comment_count": 1,
        "tags": [
            "PyCon.DE2017",
            "Keynote",
            "Python"
        ],
        "categories": [
            "Education"
        ],
        "description": "Since 2008 Susanne Mertens is working on the experimental investigation of the elusive elementary particle, the Neutrino. With the two large-scale experiments, KATRIN and LEGEND, she explores their mass and the question of whether the neutrino is its own anti-particle. Beyond that, the group of Susanne Mertens is developing a novel detector system, to search for a new type of neutrino, the so-called sterile neutrino, which could make up a large part of the dark matter of our universe.\nNeutrinos: who are you and if yes how many?\n\nThe neutrino is a strange particle: it it extremely light and flies through matter without leaving a trace. Nevertheless --due to its vast abundance-- it plays a key role as cosmological architect in the formation of galaxies in our universe. One of the missing puzzle pieces for the exact understanding of the evolution of the universe is the mass of the neutrino. The discovery of neutrino oscillations, awarded with the Nobel prize in 2015, proofs that neutrinos are not massless, but does not reveal its actual value. The Karlsruhe Tritium Neutrino (KATRIN) experiment aims at directly measuring the neutrino mass by investigating the radioactive decay of tritium with unprecedented precision. The talk will report on the mysterious neutrinos, how KATRIN will track down their mass, and which role computational methods play in the realization of such a large-scale experiment.\n\n    TU-Munich Susanne Mertens: https://www.ph.tum.de/about/people/vcard/8C819C16F9AF3575/?language=en\n    KATRIN and TRISTAN: Neutrinos and Dark Matter: https://www.mpp.mpg.de/forschung/astroteilchenphysik-und-kosmologie/katrin-und-tristan-neutrinos-und-dunkle-materie/\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/3h-6GBBF4Hg/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=3h-6GBBF4Hg",
        "original_url": "https://www.youtube.com/watch?v=3h-6GBBF4Hg"
    },
    {
        "index": 2,
        "title": "PyCon.DE 2017 Keynote Matthew Rocklin - Dask: Next Steps in Parallel Python",
        "url": "https://www.youtube.com/watch?v=rZlshXJydgQ",
        "video_id": "rZlshXJydgQ",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171128",
        "duration": 3771,
        "view_count": 4558,
        "like_count": 68,
        "comment_count": 2,
        "tags": [
            "PyCon.DE2017",
            "PyCon.DE",
            "Python",
            "Keynote"
        ],
        "categories": [
            "Education"
        ],
        "description": "Matthew is an open source software developer focusing on efficient computation and parallel computing, primarily within the Python ecosystem. He has contributed to many of the PyData libraries and today works on Dask a framework for parallel computing. Matthew holds a PhD in computer science from the University of Chicago where he focused on numerical linear algebra, task scheduling, and computer algebra.\n\nMatthew lives in Brooklyn, NY and is employed by Anaconda Inc.\n\n    Website: http://matthewrocklin.com\n    Blog: http://matthewrocklin.com/blog\n    Dask: http://dask.pydata.org/\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/rZlshXJydgQ/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=rZlshXJydgQ",
        "original_url": "https://www.youtube.com/watch?v=rZlshXJydgQ"
    },
    {
        "index": 3,
        "title": "PyCon.DE 2017 Keynote Prof. Dr. Michael Feindt - AI: Differentiating Hype and Real Value",
        "url": "https://www.youtube.com/watch?v=Ndi4hqSdBi4",
        "video_id": "Ndi4hqSdBi4",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171128",
        "duration": 4312,
        "view_count": 602,
        "like_count": 4,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "Keynote",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Prof. Dr. Michael Feindt is the brain behind Blue Yonder GmbH . His NeuroBayes algorithm was developed during his many years of scientific research at CERN. Michael is also a professor at the Karlsruhe Institute of Technology (KIT), Germany, and a lecturer at the Data Science Academy.\nArtificial Intelligence: Differentiating Hype and Real Value\n\nStarting from the evolution of biological and artificial life and intelligence examples of already existing superhuman performance of \"narrow“ AI are presented. The history of artificial neural networks nicely shows how wrong human experts can be. Conditions for and principles of predictive and prescriptive analytics, two important working horses of AI, are presented. It is explained why superhuman performance is especially possible in mass decisions under uncertainty. Their value is proven with examples from largely improved and automated public scientific research as well as decision making in enterprises.\n\nA personal view about the role of python and some selected topics that will gain much more attention like causality extraction from historic data and discrimination-free algorithms are given, before concluding on how we should face the challenges but especially the chances of AI to create a better world. \n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/Ndi4hqSdBi4/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=Ndi4hqSdBi4",
        "original_url": "https://www.youtube.com/watch?v=Ndi4hqSdBi4"
    },
    {
        "index": 4,
        "title": "PyCon.DE 2017 Lightning Talks Wednesday",
        "url": "https://www.youtube.com/watch?v=wE2-jepEZ7k",
        "video_id": "wE2-jepEZ7k",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171128",
        "duration": 3265,
        "view_count": 272,
        "like_count": 6,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Recorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/wE2-jepEZ7k/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=wE2-jepEZ7k",
        "original_url": "https://www.youtube.com/watch?v=wE2-jepEZ7k"
    },
    {
        "index": 5,
        "title": "PyCon.DE 2017 Lightning Talks Thursday",
        "url": "https://www.youtube.com/watch?v=j41vVlwFJrE",
        "video_id": "j41vVlwFJrE",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171128",
        "duration": 2891,
        "view_count": 141,
        "like_count": 1,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData",
            "Lightning Talks"
        ],
        "categories": [
            "Education"
        ],
        "description": "Recorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/j41vVlwFJrE/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=j41vVlwFJrE",
        "original_url": "https://www.youtube.com/watch?v=j41vVlwFJrE"
    },
    {
        "index": 6,
        "title": "PyCon.DE 2017 Lightning Talks Friday",
        "url": "https://www.youtube.com/watch?v=ZRIPqa9asrU",
        "video_id": "ZRIPqa9asrU",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171128",
        "duration": 2522,
        "view_count": 94,
        "like_count": 0,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData",
            "Lightning Talks"
        ],
        "categories": [
            "Education"
        ],
        "description": "Recorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/ZRIPqa9asrU/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=ZRIPqa9asrU",
        "original_url": "https://www.youtube.com/watch?v=ZRIPqa9asrU"
    },
    {
        "index": 7,
        "title": "PyCon.DE 2017 Daniel Jilg - Python in Space - The N Body Problem",
        "url": "https://www.youtube.com/watch?v=E1nx4ReTgps",
        "video_id": "E1nx4ReTgps",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171128",
        "duration": 2464,
        "view_count": 1754,
        "like_count": 37,
        "comment_count": 1,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Daniel Jilg (@breakthesystem)\nI've been an app developer, a CTO, and a Python developer, mostly at the same time. I've carried through it all a desire to learn, a love for data visualisation, and a knack to pass on what I learned. I want to learn, I want to mentor, and I want to create inclusive spaces for people to learn and mentor themselves. In my free time, I open tins for my cats (who have their own instagram account), visit various spaceports around the world, and occasionally dabble in some motor sports.\n\nAbstract\n\nTags: complexity data structures algorithms space python\n\nThe N Body Problem is a computationally complex problem that we use to predict how planets and galaxies – and everything in between – move through space. I'll show you some interesting ways to calculate it, and we'll have a look at what to do, should you find yourself in a space ship's pilot seat.\nDescription\n\nThe N Body Problem is one of those famously hard to solve problems in computer science. It describes the movement and interactions of n bodies in space (where n is usually greater 3). You need a lot of computational power to passably approximate the gravitational influences a group of celestial bodies exert on each other. Can Python do that?\n\nI'll give a super brief overview of the history of trying to predict the motion of moon, sun and the visible stars, and then we'll dive deep into the original N Body Problem, followed by creative ways to reduce the amount of calculation needed and still get a reasonable approximation. Doing this, we can see how to tackle some complex mathematic and algorithmic problems easily using Python.\n\nIf you're a Python beginner or even very early in your career as a developer, you are going to get a first impression of efficient algorithms and complex data structures, and how to measure complexity using Big O notation. If you're more advanced, you'll be treated to a hopefully interesting way of implementing the Barnes-Hutt-algorithm, and some space nerdery.\n\nOnce we have all the pieces in place, let's play with some orbits. We'll see how orbits actually behave, as opposed to what the untrained mind would expect. We can use our new algorithms to try out some manoeuvres often used by real space craft and help you be prepared for when that call from ESA or NASA finally comes, and you'll be invited to work on Python In Space!\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/E1nx4ReTgps/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=E1nx4ReTgps",
        "original_url": "https://www.youtube.com/watch?v=E1nx4ReTgps"
    },
    {
        "index": 8,
        "title": "PyCon.DE 2017 Alex Conway - Deep Learning for Computer Vision",
        "url": "https://www.youtube.com/watch?v=X4Q6C915sUY",
        "video_id": "X4Q6C915sUY",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171128",
        "duration": 2490,
        "view_count": 1860,
        "like_count": 34,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Alex Conway (@alxcnwy)\nAlex is the founder & CTO of NumberBoost, a startup that builds deep learning applications. He previously worked as a quant for a hedge fund and as a data scientist for an e-commerce company. He has an honours degree in actuarial science and a MSc in statistics. He is one of the organizers of the Cape Town Deep Learning meet-up and has built numerous computer vision systems that run at scale in production predicting labels for millions of images per day.\n\nAbstract\n\nTags: business data-science use-case deep learning ai machine learning\n\nThe state-of-the-art in image classification has skyrocketed thanks to the development of deep convolutional neural networks and increases in the amount of data and computing power available to train them. The top-5 error rate in the ImageNet competition to predict which of 1000 classes an image belongs to has plummeted from 28% error in 2010 to just 2.25% in 2017 (human level error is around 5%).\n\nIn addition to being able to classify objects in images (including not hotdogs), deep learning can be used to automatically generate captions for images, convert photos into paintings, detect cancer in pathology slide images, and help self-driving cars ‘see’.\n\nThe talk will give an overview of the cutting edge and some of the core mathematical concepts and will also include a short code-first tutorial to show how easy it is to get started using deep learning for computer vision in python…\nDescription\n\nThis talk is a crash course on convolutional neural networks and how to use them to solve 2 real-world applications at scale. The first is an image moderation system and the second is a visual similarity system where a user uploads an image of an item and the system returns visually similar items.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi/X4Q6C915sUY/maxresdefault.jpg",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=X4Q6C915sUY",
        "original_url": "https://www.youtube.com/watch?v=X4Q6C915sUY"
    },
    {
        "index": 9,
        "title": "PyCon.DE 2017 Tamara Mendt - Modern ETL-ing with Python and Airflow (and Spark)",
        "url": "https://www.youtube.com/watch?v=tcJhSaowzUI",
        "video_id": "tcJhSaowzUI",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171128",
        "duration": 1595,
        "view_count": 22996,
        "like_count": 367,
        "comment_count": 9,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Tamara Mendt (@TamaraMendt)\n\nTamara Mendt is a Data Engineer at HelloFresh, a meal kit delivery service headquartered in Berlin, and one of the top 3 tech startups to come out of Europe over the past 4 years. She devotes her time to building data pipelines and designing and maintaining the company's data infrastructure. Tamara has a computer engineering degree from her native country Venezuela, and an Erasmus Mundus Masters degree in IT for Business Intelligence. She wrote her Master thesis at the TU Berlin with the research group where Apache Flink was born. At HelloFresh she is continuing to work with distributed technologies such has Apache Hadoop, Apache Kafka and Apache Spark to cope with the scalability that the fast growing company requires for dealing with their data.\n\nAbstract\n\nTags: data data-science pipeline\n\nThe challenge of data integration is real. The sheer amount of tools that exist to address this problem is proof that organizations struggle with it. This talk will discuss the inherent challenges of data integration, and show how it can be tackled using Python and Apache Airflow and Apache Spark.\nDescription\n\nThe way organizations analyze their data has evolved very quickly since the beginning of the millennium. The development of Hadoop, and the explosion in the variety of data that companies are dealing with nowadays, has fostered the appearance of the concept of data lake, and the shift of traditional ETL (extract, transform, load), to ELT (extract, load, transform). Yet, the challenge of integrating data to obtain valuable insights still remains, and despite the hype and attention being focused on data, very few organizations have actually managed to become data driven. In this talk I will present insights into how we are currently building data pipelines using Python (as a replacement to high level ETL software), Apache Airflow as a scheduler to our coded transformations, and Apache Spark for achieve scalability. Though building data pipelines is not the only element required to become data driven, it is a crucial one, and I hope to encourage the audience to use these open source technologies in their own ETL-ing (or ELT-ing) efforts.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/tcJhSaowzUI/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=tcJhSaowzUI",
        "original_url": "https://www.youtube.com/watch?v=tcJhSaowzUI"
    },
    {
        "index": 10,
        "title": "PyCon.DE 2017 Uwe Korn - Connecting PyData to other Big Data Landscapes using Arrow and Parquet",
        "url": "https://www.youtube.com/watch?v=-IvLScEcoO8",
        "video_id": "-IvLScEcoO8",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171128",
        "duration": 1903,
        "view_count": 273,
        "like_count": 2,
        "comment_count": null,
        "tags": [
            "PyCon.DE",
            "PyCon.DE2017",
            "Python",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Uwe L. Korn (@xhochy)\n\nUwe Korn is a Data Scientist at the Karlsruhe-based RetailTec company Blue Yonder. His expertise is on building architectures for machine learning services that are scalably usable for multiple customers aiming at high service availability as well as rapid prototyping of solutions to evaluate the feasibility of his design decisions. As part of his work to provide an efficient data interchange he became a core committer to the Apache Parquet and Apache Arrow projects.\n\nAbstract\n\nTags: data-science hadoop apache arrow parquet pandas pydata\n\nWhile Python itself hosts a wide range of machine learning and data tools, other ecosystems like the Hadoop world also provide beneficial tools that can be either connected via Apache Parquet files or in memory using Arrow. This talks shows recent developments that allow interoperation at speed.\nDescription\n\nPython has a vast amount of libraries and tools in its machine learning and data analysis ecosystem. Although it is clearly in competition with R here about the leadership, the world that has sprung out of the Hadoop ecosystem has established itself in the space of data engineering and also tries to provide tools for distributed machine learning. As these stacks run in different environments and are mostly developed by distinct groups of people, using them together has been a pain. While Apache Parquet has already proven itself as the gold standard for the exchange of DataFrames serialized to files, Apache Arrow recently got traction as the in-memory format for DataFrame exchange between different ecosystems.\n\nThis talk will outline how Apache Parquet files can be used in Python and how they are structured to provide efficient DataFrame exchange. In addition to small code sample, this also includes an explanation of some interesting details of the file format. Additionally, the idea of Apache Arrow will be presented and taking Apache Spark (2.3) as an example to showcase how performance increases once DataFrames can be efficiently shared between Python and JVM processes.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/-IvLScEcoO8/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=-IvLScEcoO8",
        "original_url": "https://www.youtube.com/watch?v=-IvLScEcoO8"
    },
    {
        "index": 11,
        "title": "PyCon.DE 2017 Natalie Speiser, Jens Beyer - Data Science Project for Beginners",
        "url": "https://www.youtube.com/watch?v=4BBCqCgVDLI",
        "video_id": "4BBCqCgVDLI",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171128",
        "duration": 1848,
        "view_count": 417,
        "like_count": 4,
        "comment_count": null,
        "tags": [
            "PyCon.DE",
            "PyCon.DE2017",
            "Python",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Natalie Speiser,Jens Beyer (@natalie_lavrio)\n\nNatalie is a psychologist focused on statistics and machine learning for predictions. Jens is a pyhsicist turned consultant for IBM and d-fine and helped big companies with statistical models since 2009. Together, we founded LAVRIO.solutions and help our clients to make the most out of their data. In our recent data science projects, we faced specific hurdles which seem to be typical.\n\nAbstract\n\nTags: business data-science use-case python machine learning\n\nAI and Machine Learning are taking over the world - but how do you actually start with understanding your data and predicting events? And what kind of \"political\" trouble could you run into? With examples from real projects, we try to give you a feeling for data science projects.\nDescription\n\nWe will be talking about examples from projects we did as data science consultants. What should be organized before you go to a client (internal or external). How should the data look like. What are problems you have to face while working with the clients IT department. You get to see a rough draft of our code. It won't be a thorough manual, just our experiences and how we dealt with problems.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/4BBCqCgVDLI/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=4BBCqCgVDLI",
        "original_url": "https://www.youtube.com/watch?v=4BBCqCgVDLI"
    },
    {
        "index": 12,
        "title": "PyCon.DE 2017 Andreas Schilling - Master 2.5 GB of unstructured specification documents with ease",
        "url": "https://www.youtube.com/watch?v=g277gRcG84I",
        "video_id": "g277gRcG84I",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171128",
        "duration": 1824,
        "view_count": 274,
        "like_count": 0,
        "comment_count": null,
        "tags": [
            "PyCon.DE",
            "PyCon.DE2017",
            "Python",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Dr. Andreas Schilling is Senior Software Engineer at eXXcellent solutions. In his job, he helps customers to develop software solutions from the early stage of defining the particular requirements to developing information systems which meet their needs.\n\nBefore working at eXXcellent solutions Andreas Schilling studied Information Systems at the University of Bamberg focusing on distributed systems and information management. Thereafter, he pursued his PhD and studied collaboration dynamics in open source projects.\n\nAbstract\n\nTags: networkx pandas visualization knowledge-management analytics use-case python business\n\nHow Do you kick start a project which is based on 2.5 GB files of unstructured specification documents? To answer this question, we present our lessons learned from developing a Python based knowledge management tool which provides a lightweight and intuitive browser frontend.\nDescription\n\nIn this talk, we present lessons learned from and practical advice on how to deal with a large body of specification documents in your next project. We introduce our approach as well as code excerpts from our powerful toolset to transform a large set of unstructured and partially corrupt specification documents into structured JSON Files. Finally, we showcase a simple, yet powerful Javascript frontend which requires no additional infrastructure to present the compiled artefacts in an intuitive and responsive user interface.\n\nIn particular this talk covers the following topics:\n\n    How to make use of pywin32 to access layout and content information from partially corrupt .doc and .docx files and create simple JSON files with UTF-8 encoding.\n\n    Identify and categorize signal words in your specification.\n\n    Use pandas to compile content based recommender functionality.\n\n    Use networkx and py2cytoscape to visualize call sequences and semantic relationships in your specification.\n\n    Present the compiled artefacts and identified relationships in an easy-to-use and lightweight Javascript browser interface without any additional infrastructure (i.e. no webserver and no database server).\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/g277gRcG84I/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=g277gRcG84I",
        "original_url": "https://www.youtube.com/watch?v=g277gRcG84I"
    },
    {
        "index": 13,
        "title": "PyCon.DE 2017 Sefan Behnel - Lift your Speed Limits with Cython",
        "url": "https://www.youtube.com/watch?v=nTKQkm8U0zE",
        "video_id": "nTKQkm8U0zE",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 2520,
        "view_count": 391,
        "like_count": 5,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Stefan has been using Python since the beginning of the century and is probably best known as core developer of Cython and lxml. He spends his time joyfully writing Python and Cython code and teaching people like you how to make the best out of these tools.\n\nAbstract\n\nTags: compilation optimisation cython performance\n\nThink you can benefit from making your Python application run faster? Then come along and learn how to tune your code with Cython.\nDescription\n\nCython is not just a Python compiler or a native code wrapping tool, but also a programming language in its own right that mixes Python with C and C++. The fact that you can simply drop Python code into it and compile it gives a very low entry level into performance optimisation by statically compiling your critical code and then tuning it into fast C to make it run faster.\n\nIn this talk, I will show a few use cases where the compilation of regular Python code leads to faster execution, and present ways how to make the code run much faster.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/nTKQkm8U0zE/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=nTKQkm8U0zE",
        "original_url": "https://www.youtube.com/watch?v=nTKQkm8U0zE"
    },
    {
        "index": 14,
        "title": "PyCon.DE 2017 Axel Arnold - And now to something ELSE: Real Time Data Processing @ billiger.de",
        "url": "https://www.youtube.com/watch?v=en7XcpYxLU4",
        "video_id": "en7XcpYxLU4",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 1961,
        "view_count": 317,
        "like_count": 6,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Having started as a computational physicist, developing GPU-accelerated based molecular dynamics software for 14 years, I developed a passion for Python as scripting frontend and powerful numeric analysis tool. And NumPy makes teaching numerical math to 3rd year physics students so much easier! Eventually, I decided to make my living using Python and joined solute GmbH as Python developer. I still have a passion for high throughput systems, but now not only from developer's, but also the operations point of view.\n\nAbstract\n\nTags: use-case python business\n\nbilliger.de is one of the largest price comparison websites in Germany. In this talk, we want to share how we built the scalable, event-driven processing system which renders the products for our website using Python, Elasticsearch and redis.\nDescription\n\nbilliger.de is one of the largest price comparison websites in Germany. Tens of millions of offers need to be clustered into products, with a considerable amount of offers changing every few hours. In this talk, we want to share our experiences and the challenges we were facing on our way to building a scalable, event-driven processing system which renders the products you will eventually find on our website. We give an overview of that system, which consists of a number of services written in Python, a Redis based queueing system and an Elasticsearch cluster for storage. We tell the story of how all of this works together and how it allows us to scale storage and processing throughput according to our needs.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/en7XcpYxLU4/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=en7XcpYxLU4",
        "original_url": "https://www.youtube.com/watch?v=en7XcpYxLU4"
    },
    {
        "index": 15,
        "title": "PyCon.DE 2017 Suhas SG - Hacking the Python AST",
        "url": "https://www.youtube.com/watch?v=kaxAF542Cic",
        "video_id": "kaxAF542Cic",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 2166,
        "view_count": 1388,
        "like_count": 36,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Suhas SG (@jargnar)\n\nSuhas is a data scientist, full stack engineer, and a design hacker originally from Bangalore, India. He is currently a data engineer at Zalando SE, Berlin. Earlier, he has designed and built big data solutions for some of the Fortune 500 companies.\n\nHe is also the founder and curator of a data visualization and art museum called LifeWithData.\n\nAbstract\n\nTags: python\n\nComputer languages are a remarkable feat of human scientific engineering. In this talk, we'll look at the innards of CPython, and specifically learn how to modify and hack Abstract Syntax Trees (for world peace, of course).\nDescription\nSummary\n\nComputer languages are a remarkable feat of human scientific engineering. In this talk, we'll look at the innards of CPython, and specifically learn how to modify Abstract Syntax Trees (for world peace, of course).\nRough agenda\n\n    [ 05 mins ] --- A brief intro to languages and compilers\n    [ 05 mins ] --- CPython and parsing grammars\n    [ 10 mins ] --- Everything about Python ASTs\n    [ 15 mins ] --- Let's write a flake8 plugin (to understand ASTs better)\n    [ 05 mins ] --- Metaprogramming, code as data, possibilities and more!\n\nWhat we'll learn\n\n    We'll learn the life of a python code, and how the CPython process works.\n    We'll understand and be able to participate in conversations that include words like LL(1) parser\n    We'll make our devious minds work towards hacking into source code and modifying it with the help of ast.NodeTransformer\n    We'll learn to write a Flake8 plugin\n    A treasure map to dive into deeper waters\n\nPrerequisites:\n\n    Basics of Python, must be comfortable with lists, functions, classes\n    Basic idea / understanding of data structures\n    A mischievous mind to get hands dirty for intercepting source code and parsing ASTs\n    A bottle of water to keep yourself hydrated\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/kaxAF542Cic/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=kaxAF542Cic",
        "original_url": "https://www.youtube.com/watch?v=kaxAF542Cic"
    },
    {
        "index": 16,
        "title": "PyCon.DE 2017 Thomas Waldmann - The BorgBackup Project",
        "url": "https://www.youtube.com/watch?v=oLFMsP1GMa0",
        "video_id": "oLFMsP1GMa0",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 1820,
        "view_count": 2718,
        "like_count": 47,
        "comment_count": 1,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Thomas Waldmann (@ThomasJWaldmann)\n\nDoing Python since 2001, Linux, FOSS.\n\nProjects: MoinMoin Wiki, nsupdate.info, bepasty, BorgBackup\n\nAbstract\n\nTags: bestpractices crypto backup c cython python\n\nBorgBackup is a modern, deduplicating backup software written in Python 3.4+, Cython and C.\n\nThe talk will start with a quick presentation about the software and why you may want to use it for your backups.\n\nThen, I will show how we run the software project: Tools, Services, Best Practices.\nDescription\n\nHomepage: www.borgbackup.org\n\nAbout the software:\n\n    Feature set\n    Code\n    Security\n    Safety\n    Crypto\n    Compression\n    Deduplication\n    The Fork from attic-backup project (and what happened afterwards)\n    Now and Future\n    How you can help\n\nAbout the project:\n\n    Github\n    Sphinx and ReadTheDocs\n    Asciinema\n    Mailing list on python.org, IRC on freenode\n    Testing: pytest, tox, travis-ci, vagrant, pyenv\n    \"binary\" Releases with PyInstaller\n    Automatic versioning: setuptools_scm\n    Secure releasing with gpg signature\n    Python / Cython / C - when to use what\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi/oLFMsP1GMa0/hqdefault.jpg",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=oLFMsP1GMa0",
        "original_url": "https://www.youtube.com/watch?v=oLFMsP1GMa0"
    },
    {
        "index": 17,
        "title": "PyCon.DE 2017 Miroslav Šedivý - Vim your Python, Python your Vim",
        "url": "https://www.youtube.com/watch?v=prnndyNV60w",
        "video_id": "prnndyNV60w",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 1770,
        "view_count": 3701,
        "like_count": 36,
        "comment_count": 3,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Miroslav Šedivý (@eumiro)\n\nI'm bad at writing my bio. https://www.linkedin.com/in/šedivý/\n\nAbstract\n\nTags: ide neovim vim editor python\n\nWhat do you use to write source code, docs, books or e-mails? Single brain, single pair of hands, single keyboard, but a different keyboard layout for each language and a different text editor for each purpose?\nDescription\n\nDo you use an IDE to work on Python code, then switch to an e-mail client to write a message, then open a different editor to work on the documentation and finally ssh to a remote server to edit a configuration file? Do you switch languages frequently or are your colleagues named Müller, François, Muñoz or even Šedivý?\n\nI'll show you how I am happily typing in several languages on a single standard US keyboard layout and why my CapsLock became so useful.\n\nI'll show you how I use a single editor on all my machines to produce all sorts of text, especially Python code with a few useful plugins.\n\nI'll show you my own plugin written in Python to hack my e-mails far beyond imagination.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi/prnndyNV60w/hqdefault.jpg",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=prnndyNV60w",
        "original_url": "https://www.youtube.com/watch?v=prnndyNV60w"
    },
    {
        "index": 18,
        "title": "PyCon.DE 2017 Raphael Michel - Plugin ecosystems for Python web-applications",
        "url": "https://www.youtube.com/watch?v=5NxRdzLTFik",
        "video_id": "5NxRdzLTFik",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 1610,
        "view_count": 386,
        "like_count": 8,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Raphael Michel (@rami)\n\nI‘m a software developer working with the web for more than ten years and with Python and Django for over four years. I work as a freelancer and maintain multiple open source projects. The most notable Django-based one is pretix, a full-featured and free conference ticketing software.\n\nAbstract\n\nTags: django python web\n\nThe power of some popular web applications like WordPress comes from a flexible plugin system. This talk will show how to implement such plugin architectures for Python web applications including real-world examples. I'll give examples with Django, but the important bits aren't Django-specific.\nDescription\n\nIt is probably common sense by now that modular architectures win over monolithic architectures in a number of ways. Django gives us reusable apps, but just because they are reusable doesn’t mean they automatically integrate nicely with each other and a Python programmer is still required to glue them together. In this talk I will show how to build a rich Django application that provides a well-defined plugin API. The API will allow plugins to seamlessly integrate with the main application. The talk will show various patterns around URL routing, Django‘s signal system and the use of setuptools to provide auto-detection of installed plugins. Installing a plugin should not be much more complicated than a simple pip install.\n\nDjango is only used as an example, most parts could be easily transferred to other web frameworks as well. The explained architecture has been in use for some years in the pretix open source project and made it possible to us to grow a rich plugin ecosystem.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/5NxRdzLTFik/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=5NxRdzLTFik",
        "original_url": "https://www.youtube.com/watch?v=5NxRdzLTFik"
    },
    {
        "index": 19,
        "title": "PyCon.DE 2017 Christian Staudt - The Python Ecosystem for Data Science: A Guided Tour",
        "url": "https://www.youtube.com/watch?v=BIWcciNeMm0",
        "video_id": "BIWcciNeMm0",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 2243,
        "view_count": 251,
        "like_count": 2,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Christian Staudt (@C_L_Staudt)\n\nI am an independent data scientist with a background in computer science, in-depth in algorithms, data analysis, high-performance computing and software engineering. My current interests include machine learning and data visualization.\n\nAbstract\n\nTags: use-case business ai analytics data-science python machine learning\n\nPythonistas have access to an extensive collection of tools for data analysis. The space of tools is best understood as an ecosystem: Libraries build upon each other, and a good library fills an ecological niche by doing certain jobs well. This is a guided tour of the Python data science ecosystem.\nDescription\n\nThe Python Ecosystem for Data Science: A Guided Tour\n\nPython is on its way to becoming the lingua franca of data science, and Pythonistas have access to an impressive and extensive collection of tools for data analysis. Here, a data scientist needs to see the forest for the trees: The space of tools is best understood as an ecosystem, where libraries build upon each other, and where a good library fills an ecological niche by doing certain jobs well. This talk is a guided tour of the Python data science ecosystem. More than a list of libraries, it aims to provide some structure, classing tools by type of data, size of data, and type of analysis. In our tour, we visit a number of areas, including working with tabular data (numpy, pandas, dask, ...) and graph data (e.g. networkx), statistics (e.g. statsmodels), machine learning (scikit-learn, ...), data visualization (matplotlib, seaborn, bokeh, ...). Aspiring data scientists, and everyone else working with data, should find this useful for selecting the right tools for their next data-driven project.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/BIWcciNeMm0/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=BIWcciNeMm0",
        "original_url": "https://www.youtube.com/watch?v=BIWcciNeMm0"
    },
    {
        "index": 20,
        "title": "PyCon.DE 2017 Joey Faulkner - Rasa: open source conversational AI to build next generation chatbots",
        "url": "https://www.youtube.com/watch?v=LEFF7-_uh3M",
        "video_id": "LEFF7-_uh3M",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 1832,
        "view_count": 1370,
        "like_count": 25,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Joey Faulkner (@JoeyMFaulkner)\n\nI am a PhD student in machine learning/astronomy, and an AI researcher at Rasa. We make open source libraries for conversational AI.\n\nAbstract\n\nTags: python machine learning deep learning open source chatbot natural language ai business\n\nSoon you will primarily communicate with your computer through conversation. At Rasa, we believe that this revolution in user experience should be available to everyone. In this spirit we have developed open source tools that use machine learning to make chatbots in a developer-friendly interface.\nDescription\n\nThe revolution-in-waiting for user experience is conversational AI, and this revolution should be available to everyone. The ability to communicate with your computer via a fulfilling and useful conversation will change the way we approach user interaction. Recent advances in machine learning have made this goal not only possible to reach, but possible to bring to the developer community at large. At Rasa we have developed a set of open-source Python libraries which can comprehend natural language and handle complex, interesting conversations. We utilise deep learning to bypass existing rigid conversational norms (state machines, if/else statements, etc.) and allow developers to make awesome bots simply by talking to them.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi/LEFF7-_uh3M/hqdefault.jpg",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=LEFF7-_uh3M",
        "original_url": "https://www.youtube.com/watch?v=LEFF7-_uh3M"
    },
    {
        "index": 21,
        "title": "PyCon.DE 2017 Hendrik Niemeyer - Synthetic Data for Machine Learning Applications",
        "url": "https://www.youtube.com/watch?v=riT9KTkBj0E",
        "video_id": "riT9KTkBj0E",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 2710,
        "view_count": 1942,
        "like_count": 25,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Dr. Hendrik Niemeyer (@hniemeye)\n\nData Scientist working on predictive analytics with data from pipeline inspection measurements.\n\nAbstract\n\nTags: data-science python machine learning ai\n\nIn this talk I will show how we use real and synthetic data to create successful models for risk assessing pipeline anomalies. The main focus is the estimation of the difference in the statistical properties of real and generated data by machine learning methods.\nDescription\n\nROSEN provides predictive analytics for pipelines by detecting and risk assessing anomalies from data gathered by inline inspection measurement devices. Due to budget reasons (pipelines need to be dug up to get acess) ground truth data for machine learning applications in this field are usually scarce, imbalanced and not available for all existing configurations of measurement devices. This creates the need for synthetic data (using FEM simulations and unsupervised learning algorithms) in order to be able to create successful models.\n\nBut a naive mixture of real-world and synthetic samples in a model does not necessarily yield to an increased predictive performance because of differences in the statistical distributions in feature space. I will show how we evaluate the use of synthetic data besides simple visual inspection. Manifold learning (e.g. TSNE) can be used to gain an insight whether real and generated data are inherently different.\nQuantitative approaches like classifiers trained to discriminate between these types of data provide a non visual insight whether a \"synthetic gap\" in the feature distributions exists.\n\nIf the synthetic data is useful for model building careful considerations have to be applied when constructing cross validation folds and test sets to prevent biased estimates of the model performance.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/riT9KTkBj0E/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=riT9KTkBj0E",
        "original_url": "https://www.youtube.com/watch?v=riT9KTkBj0E"
    },
    {
        "index": 22,
        "title": "PyCon.DE 2017 Thuy Le - Sport analysis with Python",
        "url": "https://www.youtube.com/watch?v=Fl6YFdf37IE",
        "video_id": "Fl6YFdf37IE",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 1408,
        "view_count": 756,
        "like_count": 3,
        "comment_count": 1,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Thuy Le\n\nAbstract\n\nTags: devops analytics data-science python\n\nSport analysis with Python and visualize data with tableau. We have sample data of a team in football match (name of players, positions of players, velocities of players) which are recorded in every 20 millisecond. We use python to analysis and Tableau to visualize the activities of each player\nDescription\n\n    Give an example data of the IoT sport case for instance the information of football match of a team (the positions, velocities of each player with are recorded in every 20 millisecond).\n    We use Python to analysis and processing data (calculate the match time, analyst the activities of each player such as time in the bench, time in the pitch, ... )\n    We use Tableau to visualize data\n\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/Fl6YFdf37IE/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=Fl6YFdf37IE",
        "original_url": "https://www.youtube.com/watch?v=Fl6YFdf37IE"
    },
    {
        "index": 23,
        "title": "PyCon.DE 2017 Heidi Thorpe - The Mustache Movement",
        "url": "https://www.youtube.com/watch?v=9lVbpzd1hWk",
        "video_id": "9lVbpzd1hWk",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 1714,
        "view_count": 137,
        "like_count": 3,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Heidi Thorpe\n\nHeidi graduated with Honors in Chemistry from the University of Leeds before taking up a position as an Industrial Chemist with ICI (ORICA). This led to a career in software development working in UK, Aus and USA. In 2000 she became an author by the publication by Addison Wesley of Oracle8i tuning and administration: the essential reference ASIN: B007EIZGOG this was a sales success and translated into German. In her spare time she uses python to write software that allows her to train object classifiers to steal mens' mustaches and put them on the face of Taylor Swift. For this talk she will give a simple introduction to Generative Adversarial Networks using learned \"mustache-ness\" as an example, implemented with existing python modules\n\nPyconAu 2016 and PyconHK 2016 is her speaking experience https://www.youtube.com/watch?v=70JAm03IBFI\n\nAbstract\n\nTags: python machine learning torch scikit-learn numpy neural networks\n\nGenerative Adversarial Networks (GANs) are a class of neural networks which are powerful and flexible tools. A common application is image generation. I would like to give a simple introduction to GANs using existing python modules and an example of how \"mustache-ness\" can be learned and applied.\nDescription\n\nThe objective is to showcase the power and flexibility of combining preexisting python modules and applying them to the field of image processing and machine learning. Learning what constitutes \"Mustache-ness\" in a straight forward and fun example of applying the technology in a 30 min presentation\n\nAttendees will learn tips on image processing, as well as training their own neural network.\n\nAn analogy can be considered as a relation between forger and detective. The task of the forger is to create fake examples of original paintings by famous artists. If the fake can pass as an original the the forger can exchange the fake for money. The task of the detective is to catch the forgers. He does this by knowing what are the properties that set the original artist apart and what sort of picture he would create. The detective uses this knowledge to determine whether or not the image is real or fake. From an introduction of what a Generative Adversarial Network is, how they work and how they can be used to generate fake images of mustaches by determining what constitutes \"mustache-ness\"\n\nYou will learn that there are two main components of a GAN – Generator Neural Network and Discriminator Neural Network.\n\nThe Generator Network takes an random input and tries to generate a sample of data.\n\nIt then generates data which is subsequently fed into a discriminator network D(x). The task of Discriminator Network is to take input either from the real data or from the generator and try to predict whether the input is real or generated.\n\nD(x) solves a binary classification problem using sigmoid function giving output in the range 0 to 1.\n\nYou will learn how to define the problem. Do you want to generate fake images or fake text. Having defined the problem you can then collect data for it. You will define how your GAN should look. In our example of \"mustache-ness\" you will choose a convolutional neural network.\n\nYou will see the effect of training the Discriminator on real data for n epochs. Using examples of real mustaches we will generate fake ones. We will see generated fake inputs for the generator and the effect of training the discriminator on fake data. The discriminator will correctly predict them as fake. Subsequently we will see how to train the generator with the output of discriminator. Now we can see a trained generator that can fool the discriminator. Generating fake mustaches that are indistinguishable from real.\n\nAttendees will learn tips on image processing, as well as training your own convolutional neural network.\n\nShowcasing the power of python in a fun, lighthearted way. Hopefully, being informative and entertaining\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/9lVbpzd1hWk/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=9lVbpzd1hWk",
        "original_url": "https://www.youtube.com/watch?v=9lVbpzd1hWk"
    },
    {
        "index": 24,
        "title": "PyCon.DE 2017 Florian Thole - Theoretical physics with sympy",
        "url": "https://www.youtube.com/watch?v=Ax0et1ZOOTc",
        "video_id": "Ax0et1ZOOTc",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171129",
        "duration": 1981,
        "view_count": 831,
        "like_count": 10,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Florian Thöle (@florian_thl)\n\nPhD student in Computational Materials Science. Enthusiastic about teaching. Instructor for Software Carpentry.\n\nAbstract\n\nTags: physics science sympy\n\nIn this talk, I will introduce the basics of sympy. Using a simple model system in magnetism, we'll play around with simplifications, then do a bit of numerical optimization and in the end make psychedelic-looking figures.\nDescription\n\nI will introduce the basic functionalities of the sympy package to do symbolic computing, with a special focus on vector and matrix operations. Then, I'll briefly explain a real-world model from the description of 2D layered magnetic materials and use sympy to deal with the resulting expressions. We'll evaluate those expressions to visualize the results of the model and obtain a numerical estimate of a transition point.\n\nThe aim of this talk is to give a light-hearted introduction into the world of symbolic computing to someone who has more fun working with computers than pen and paper.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/Ax0et1ZOOTc/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=Ax0et1ZOOTc",
        "original_url": "https://www.youtube.com/watch?v=Ax0et1ZOOTc"
    },
    {
        "index": 25,
        "title": "PyCon.DE 2017 Carsten Pohl - Platform intrusion detection with deep learning",
        "url": "https://www.youtube.com/watch?v=dXqBuZi7JOE",
        "video_id": "dXqBuZi7JOE",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 2612,
        "view_count": 312,
        "like_count": 5,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Carsten Pohl (@carstenpohl)\n\nBefore joining REWE digital I worked for Zalando, Fraunhofer IML. My first computer was a VIC-20. I develop professionally software since the 90ies. I am one of the main developers of the new Big data platform of REWE digital, and have deep knowledge about all aspects of the platform. And I have no favorite member of One Direction.\n\nAbstract\n\nTags: python use-case deep learning e-commerce\n\nshop.rewe.de is not only visited by human customers, but also by machines. We have built a deep learning platform using python with Keras, Tensorflow, on the Google infrastructure. In this talk we would like to show you how python is used in practice, supporting 2,5 million visitors each day.\nDescription\n\nshop.rewe.de is visited over 2 million times each day. Every visitor is producing thousands of requests in our micro service architecture. We are trying to give the best shopping experience for our customers, and try to keep our platform safe from bad bots. This is partly done by rule sets, and furthermore done by a platform that uses machine learning to classify bad behaviour. In this talk I would like to present our architecture that is not only able to fulfill this use case, but enables our data scientists in general to use our big data platform. the main scope will be the presentation of the use case. I will cover:\n\n    different microservices written in python using flask, google bigquery, tensorflow and keras\n    scaling this microservices with kubernetes, that automatically starts more predictors in case of higher load\n    implementation of quantifiers written in python, that generates data suitable for neural networks using numpy\n    examples from the real world behaviour of this plattform with lessons learned\n    examples how we feature engineered the data by analysing the data stored\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/dXqBuZi7JOE/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=dXqBuZi7JOE",
        "original_url": "https://www.youtube.com/watch?v=dXqBuZi7JOE"
    },
    {
        "index": 26,
        "title": "PyCon.DE 2017 Yves Hilpisch - Why Python Has Taken Over Finance",
        "url": "https://www.youtube.com/watch?v=aXh7K6cFA8g",
        "video_id": "aXh7K6cFA8g",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1968,
        "view_count": 971,
        "like_count": 12,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Yves Hilpisch (@dyjh)\n\nThe Python Quant http://hilpisch.com\n\nAbstract\n\nTags: algorithms performance finance python\n\nNot too long ago, the finance field was dominated by compiled languages, such as C or C++, since they were considered to be the right choice for the implementation of computationally demanding algorithms. This talk explains why Python has become No 1 in the field.\nDescription\n\nNot too long ago, the finance field was dominated by compiled languages, such as C or C++, since they were considered to be the right choice for the implementation of computationally demanding algorithms.\n\nOver the last couple of years, Python and its fast growing eco-system of (data analysis) packages have made it today's language of choice for the financial industry.\n\nThis talk illustrates, based on some simple examples, the benefits of working with a high level programming language, such as Python, for the tasks at hand in finance. The examples illustrate that really concise Python code can achieve high performance results.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/aXh7K6cFA8g/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=aXh7K6cFA8g",
        "original_url": "https://www.youtube.com/watch?v=aXh7K6cFA8g"
    },
    {
        "index": 27,
        "title": "PyCon.DE 2017 Romain Dorgueil - Simple Data Engineering in python 3.5+ with Bonobo",
        "url": "https://www.youtube.com/watch?v=3agWJTRn2cc",
        "video_id": "3agWJTRn2cc",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1946,
        "view_count": 641,
        "like_count": 5,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Romain Dorgueil (@rdorgueil)\n\nDeveloper, sysadmin, technical team builder, founder of two companies), advisor.\n\nCurrently helping start-ups to achieve more with less in our acceleration programs in Paris, and in charge of our product development activities.\n\nSometimes, I play go and make music, but not at the same time.\n\nAbstract\n\nTags: Python Business Data-Engineering ETL Simple Bonobo\n\nSimple is better than complex, and that's True for data pipelines, too.\n\nBonobo is a python 3.5+ tool used to write and monitor data pipelines. It’s plain, simple, modern, and atomic python.\n\nThis talk is a practical encounter, from zero to a complete data pipeline.\n\nSpoiler : no «big data» here.\nDescription\n\nSimple is better than complex, right? That’s true for data pipelines too.\n\nFor the last 5 years, I hacked together extract-transform-load (ETL) processes in various different positions (ETL is just a fancy term for «bunch of things that take data somewhere and put it elsewhere, eventually transformed»).\n\nI did it as a founder, as a consultant, as a technical co-founder, for some side projects, big corporates and small side projects.\n\nIn each case, I felt frustrated with the tools available, and in some serious cases, I had to hack things myself to get the job done. Bonobo is the repackaging of my past experiences for python 3.5+, and grasping the basics should not take more than the length of the presentation.\n\nOutline (subject to small changes, for the greater good) :\n\n    INTRO : The ETL market, why a new tool, what it is, what it is not.\n    Basics and concepts.\n    Simple example.\n    Complete data pipeline example, using SQL, RDF and a small Django frontend.\n    OUTRO : A glimpse at the future.\n    Q&A\n\nBonobo is the glue you need to tie together regular functions in a transformation graph (think unix pipes). Execution strategies are abstracted so you can focus on the real operations. As a result, you can engineer simple and testable systems, using the same good computer development practices as you use in .\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/3agWJTRn2cc/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=3agWJTRn2cc",
        "original_url": "https://www.youtube.com/watch?v=3agWJTRn2cc"
    },
    {
        "index": 28,
        "title": "PyCon.DE 2017 Sebastian Dreßler - High-Performance Ingestion with Python and Swarm64DB",
        "url": "https://www.youtube.com/watch?v=L4EdHKLB_08",
        "video_id": "L4EdHKLB_08",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 2459,
        "view_count": 239,
        "like_count": 3,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Sebastian Dreßler\n\nAbstract\n\nTags: python\n\nSwarm64DB is a hardware-accelerated plugin for PostgreSQL and other RDBMS. By using Swarm64DB in combination with PostgreSQL, Python and the right scaling mechanism, we are able to push the ingestion throughput into areas where Python can easily compete with compiled languages. The talk highlights the architecture of our solution and showcases a real world use-case..\nDescription\n\nIn August 2016, a benchmark about asyncpg, a new PostgreSQL client library for Python was published. It highlighted ingestion speeds of up to 900k rows per second for a synthetic benchmark with a single byte per row. However, in realistic use-cases the ingestion speed is usually below that. Surprisingly it is not Python which is the limiting factor but rather the database itself. To make traditional RDBMS ready for Big Data, high performing OLAP and fast analysis on high speed data streams, Swarm64 created Swarm64DB, a hardware-accelerated plugin for PostgreSQL and other RDBMS. By using Swarm64DB in combination with PostgreSQL, Python and the right scaling mechanism, we are able to push the ingestion throughput into areas where Python can easily compete with compiled languages. The talk highlights the architecture of our solution and showcases a real world use-case.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/L4EdHKLB_08/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=L4EdHKLB_08",
        "original_url": "https://www.youtube.com/watch?v=L4EdHKLB_08"
    },
    {
        "index": 29,
        "title": "PyCon.DE 2017 Michael König - Turbodbc: Turbocharged database access for data scientists",
        "url": "https://www.youtube.com/watch?v=B-uj8EDcjLY",
        "video_id": "B-uj8EDcjLY",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1952,
        "view_count": 884,
        "like_count": 16,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Michael König (@turbodbc)\n\nMichael is a senior software engineer at Blue Yonder GmbH. He holds a PhD in physics, practices test-driven development, and digs Clean Code in C++ and Python. In the last five years, he invested more money in table tennis gear than in smartphones.\n\nAbstract\n\nTags: numpy database python data-science analytics\n\nPython's database API 2.0 is well suited for transactional database workflows, but not so much for column-heavy data science. This talk explains how the ODBC-based turbodbc database module extends this API with first-class, efficient support for familiar NumPy and Apache Arrow data structures.\nDescription\n\nThis talk introduces the open source Python database module turbodbc. It uses standard ODBC drivers to connect with virtually any database and is a viable (and often faster) alternative to \"native\" Python drivers.\n\nBriefly recounting the painful story of how data scientists previously used our analytics database, I explain why turbodbc was created and what distinguishes it from other ODBC modules. Sketching the flow of data from databases via drivers and Python modules to consumable Python objects, I motivate a few extensions to the standard database API 2.0 that turbodbc has made. These extensions heavily use NumPy arrays and Apache Arrow tables to provide data scientists with both familiar and efficient binary data structures they can further work on. I conclude my talk with benchmark results for a few databases.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/B-uj8EDcjLY/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=B-uj8EDcjLY",
        "original_url": "https://www.youtube.com/watch?v=B-uj8EDcjLY"
    },
    {
        "index": 30,
        "title": "PyCon.DE 2017 Alexander Hendorf - Effective Data Analysis with Pandas Indexes",
        "url": "https://www.youtube.com/watch?v=-E2VTtdwT9U",
        "video_id": "-E2VTtdwT9U",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1851,
        "view_count": 285,
        "like_count": 1,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Alexander Hendorf\n\nAbstract\n\nTags: machine learning analytics data-science business analytics\n\nPandas is the Swiss-Multipurpose Knife for Data Analysis in Python. In this talk we will look deeper into how to gain productivity utilizing Pandas powerful indexing and make advanced analytics a piece of cake. Pandas features multiple index types. This talk will give you a deep insight into the Pandas indexes and showcase the handiness of special Indexes as the TimeSeriesIndex.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/-E2VTtdwT9U/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=-E2VTtdwT9U",
        "original_url": "https://www.youtube.com/watch?v=-E2VTtdwT9U"
    },
    {
        "index": 31,
        "title": "PyCon.DE 2017 Hardy Erlinger - Programming the Web of Things with Python and MicroPython",
        "url": "https://www.youtube.com/watch?v=JtsLlYvcRJI",
        "video_id": "JtsLlYvcRJI",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 2782,
        "view_count": 216,
        "like_count": 1,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Hardy Erlinger is an experienced freelance developer from Munich. He has been writing web applications for 15 years using a lot of different technologies along the way. He has been actively involved with technical communities for most of his career and likes to experiment with electronic devices in his mad scientist lab.\n\nAbstract\n\nTags: micropython python3\n\nIn this session you will get a gentle introduction to the ever-expanding world of small programmable devices: learn to use single board computers and microcontrollers to connect to sensors and talk to APIs - all using Python or MicroPython, a subset of Python 3 for use in constrained environments.\nDescription\n\nSmall single board computers such as the Raspberry Pi have made physical computing available to interested makers around the world. It is highly affordable, runs Linux and can be programmed using Python. But if all you want is to read a few sensors, the Pi's specs are overkill; and once your sensors are out in the field (sometimes even literally), power consumption suddenly becomes an issue. Many specialized devices with very low power requirements are available but historically you were constrained to using C to develop for them. Enter MicroPython, a subset of Python 3 designed for use on tiny devices with very limited resources.\n\nIn this session you will be introduced to the basics of physical computing and the joys (and some of the pains) of writing Python code for devices that weigh less than a piece of chewing gum but still have the power to communicate with web APIs, act as wireless access points, even host a small web server, and more.\n\nThe purpose of this session is to share some of the fun and excitement that can be had doing physical computing with Python. No prior experience with electronics is assumed or required.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/JtsLlYvcRJI/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=JtsLlYvcRJI",
        "original_url": "https://www.youtube.com/watch?v=JtsLlYvcRJI"
    },
    {
        "index": 32,
        "title": "PyCon.DE 2017 Jens Nie,Peer Wagner - Empowered by Python - A success story",
        "url": "https://www.youtube.com/watch?v=2Ku3tV3QQ3M",
        "video_id": "2Ku3tV3QQ3M",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1790,
        "view_count": 409,
        "like_count": 6,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Jens Nie,Peer Wagner (@jneines)\n\nJens works primarily as a Technology Scout focusing on Big Data and Data Science at the ROSEN Technology & Research Center in Lingen, Germany. In this role he screens for latest developments in these areas and evaluates and demonstrates their possible use and benefit for the company. Looking back to more than 20 years in using Python and Linux, he considers him to be an Open Source enthusiast and committed Python advocate.\n\nAbstract\n\nTags: use-case python business\n\nIntroducing a new programming language in a company is always a daring task, usually involving a lot of effort and the will for change.\n\nWe'd like to take you on a journey reflecting eight years of challenges, solutions and success ending in a best practice guide helping you to achieve the same.\nDescription\n\nROSEN is most recognized as a technology leader in the pipeline inspection business. To steadily achieve the best in class service for our customers, we are building and using the most sophisticated tools in the industry to perform the inspection services. This often results in very large volumes of data which have to be handled and analyzed.\n\nAs a consequence, we were always out to search for the latest technology and state of the art tools to aid us.\n\nAt a point where the most traditionally used tools for data analysis no longer scaled a handful of pioneers were brave enough to introduce a massive change in the scientific tool chain. This involved to address driving laboratory equipment, data processing, analysis and display, handling large volumes of data efficiently, building attractive graphical user interfaces, integrate with existing software applications and prepare for modern data analysis concepts based on machine learning to ensure both increasing result quality and faster data analysis.\n\nWe've found Python to be the ideal foundation to achieve all this and started off with a few initial projects in order to prove. It turned out Python would be a real game changer for us.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/2Ku3tV3QQ3M/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=2Ku3tV3QQ3M",
        "original_url": "https://www.youtube.com/watch?v=2Ku3tV3QQ3M"
    },
    {
        "index": 33,
        "title": "PyCon.DE 2017 Anne Matthies - Keeping the grip on decoupled code using CLIs",
        "url": "https://www.youtube.com/watch?v=F20vrgQCFMs",
        "video_id": "F20vrgQCFMs",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1256,
        "view_count": 130,
        "like_count": 0,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Anne Matthies (@babeltron)\n\nAnne Matthies has been coding data stuff professionally since 1996. She switched to Python 2 in 2000, to Python 3 in 2015. Currently, she’s working at Babbel, Berlin, responsible for building and operating the data platform – and developing the next generation.\n\nAbstract\n\nTags: pydata devops cli python\n\nSo you’ve decoupled your code monolith into all those micro chunks. When someone asks „How can I…“ you want to answer: „That’s easy! We’ve built that.“ Actually, you’ve built all parts needed for that. Who plugs them together? And how?\nDescription\nKeeping the grip on decoupled code using CLIs\n\nSo you’ve decoupled your monolith spaghetti code into micro chunks. You’ve switched to infrastructure as code, and you’re confident that it scales horizontally. Your data pipelines are pretty resilient, your CI pipeline runs tests on every single git push.\n\nAnd then, you get a new team member. Or your CTO wants to plot data of his brandnew sandbox project that isn’t integrated into your pipelines. Or someone just asks „How can I…“ and you want to answer: „That’s easy! We’ve built that… – Well, actually, we’ve built all parts needed for that.“ Who plugs them together? And how?\n\nIn my talk, I’d like to show how lightweight CLIs can be Ariadne Threads through the labyrinth of micro components. How at Babbel we use conda, setuptools entrypoints and simple CLI scripts to keep the grip on our data platform code chunks\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/F20vrgQCFMs/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=F20vrgQCFMs",
        "original_url": "https://www.youtube.com/watch?v=F20vrgQCFMs"
    },
    {
        "index": 34,
        "title": "PyCon.DE 2017 Maximilian Wilhelm - Building your own SDN with Debian Linux, Salt Stack and Python",
        "url": "https://www.youtube.com/watch?v=yH_0hptXL94",
        "video_id": "yH_0hptXL94",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 2228,
        "view_count": 509,
        "like_count": 5,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Maximilian Wilhelm (@BarbarossaTM)\n\nBy day Maximilian Wilhelm is working as a Senior Infrastructure Architect in the central computing department of the University of Paderborn, by night he's hacking on the infrastructure of the Freifunk Hochstift network and some Open Source projects. Since the early 2000s he has a heart for Linux and Open Source, developed a weaknes for networking, IPv6 and routing a long while a go and has beed a speaker and tutor at the #Routingdays. Lately he got his hands dirty with ifupdown2, VXLAN, Linux VRFs, infrastructure automation with Salt Stack and \"kommunistischen Frickelnetzen\" and is afraid of SDNs ever since. In his spare time he likes playing piano and the organ, taking pictures of natures and cute animals, and trying to stay on the board while Windsurfing.\n\nAbstract\n\nTags: SDN SaltStack Linux Debian Networking devops use-case netops netdevops\n\nIn this talk you will get an overview about some awesome features of comtemporary Linux networking, how to easily integrate them with some cool open source tools, and glueing all this together with Salt Stack and some Python to get your very own SDN controller for a service-provider style network.\nDescription\n\nTopics like Infrastructure Automation / Orchestration, Cloud, and Software Defined Networks are on everyones tongue and nearly all network vendors who think highly of themselves provide products and maybe even solutions in this sphere of buzzwords.\n\nWithin the last years there has been a paradigm shift towards host and segment routing – think »IP Fabric« – as well as a focus on open protocols and standards like OSPF, IS-IS, BGP & MPLS not only in the data center. This even brought us some new standards like VXLAN and a bunch of open source based “open networking” platforms. Now we aren't always locked to the operating systems of a networking vendor but can choose the control plane software from a variety of Linux based solutions which can be managed and orchestrated by lots of different means.\n\nThanks to the Linux basis and the Open Source spirit of some vendors, some features (VRFs, MPLS forwarding plane, …) today are part of the upstream Linux kernel and available for everyone! Most notable are the contributions of the Debian Linux based platform from Cumulus Networks, which include the VRF support for Linux, some MPLS patches for FRR and ifupdown2 (which is written in Python :-)).\n\nPutting a bunch of these technologies and ideas together will open up a lot of powerful options for building low budget yet mighty networks. This talk will lay out how to build a SDN based service provide like infrastructure with the help of Salt Stack, some 1000 lines of Python and a bunch of affordable hardware where overlay networks and anycast aren't things to be scared of. The Freifunk Hochstift network and server infrastructure will be used as an example.\n\nThe target audience mainly consists of (Linux-) system and network engineers / architects, who already have some experience with the other world. A positive attitude towards automation and magic is a plus.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/yH_0hptXL94/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=yH_0hptXL94",
        "original_url": "https://www.youtube.com/watch?v=yH_0hptXL94"
    },
    {
        "index": 35,
        "title": "PyCon.DE 2017 Björn Meier - No Compromise: Use Ansible properly or stick to your scripts",
        "url": "https://www.youtube.com/watch?v=7qipNlReXYg",
        "video_id": "7qipNlReXYg",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1692,
        "view_count": 290,
        "like_count": 1,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Bjoern Meier\n\nBjoern is a software engineer at Blue Yonder GmbH since 2016 after graduating in Computer Science. More correctly you could say he is a DevOps engineer at Blue Yonder where he is developing and operating - among other things - the services for the external data interfaces, preprocessing and data storage to enable the data scientists to run their prediction models. He loves the versatility and ecosystem of python to write e.g. production web apps, data analysis tools or operational scripts. If there was more free time he would like to spent it to dive deeper into functional programming languages like elixir to have a different view on things.\n\nAbstract\n\nTags: infrasturcture business devops\n\nWhat you do in Ansible should be clean an simple. What we did was not. So I will show what we did wrong but also what we have changed or still have to, to make our life easier again. But I will also show how we progressively utilize Ansible to deploy our Data Science infrastructure.\nDescription\n\nAnsible should help you to orchestrate your systems, automate the deployments and set up well defined infrastructures. But if you want to make something work quickly in Ansible the chances are high that you fall back to shell/command tasks, the mother of all evil. Those tasks usually prevent you from running dry runs where you would see the upcoming changes and you prevent Ansible to shine. So, we went blindly into every deployment and hoped the best. But we wanted to see what would change, we wanted to make ansible --check work again and therefore in this talk I will show you what we did wrong and what we changed to get there.\n\nMore precisely, I will show what you can do to replace the nasty shell tasks with proper modules, plugins and filters, how they are developed, tested and included in your project.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/7qipNlReXYg/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=7qipNlReXYg",
        "original_url": "https://www.youtube.com/watch?v=7qipNlReXYg"
    },
    {
        "index": 36,
        "title": "PyCon.DE 2017 Eberhard Hansis - Data Plumbing 101 - ETL Pipelines for Everyday Projects",
        "url": "https://www.youtube.com/watch?v=D7fYa0NrCuE",
        "video_id": "D7fYa0NrCuE",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1676,
        "view_count": 503,
        "like_count": 7,
        "comment_count": 1,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Eberhard Hansis\n\nI have been writing code for more than two thirds of my life, mostly for scientific computing and data analysis. In the past couple of years, I have worked on a range of different data science projects, all using Python at their core. During this time, I repeatedly was tasked with making data usable by joining multiple sources into a clearly defined data model. Once you have done that, it is amazing how much real-life value you can generate with little more than a bit of statistics and visualization. The world is full of underused data, let’s change that!\n\nAbstract\n\nTags: data-science analytics python\n\nThere is no data science without ETL! This presentation is about implementing maintainable data integration for your projects. We will have a first look a ‘Ozelot’, a library based on Luigi and SQLAlchemy that helps you get started with building ETL pipelines.\nDescription\nETL, the hard way\n\nYou are starting a new data science project, and you can’t wait to perform some machine learning magic. However, before getting to ML, you have to deal with its ugly sibling: ETL. Extracting, transforming and loading data (or, more generally, data integration) is an indispensable first step in almost any data project.\n\nIn your project you will, most likely, have to extract data from various sources, clean it, link it and prepare it to your needs. You will start writing a first data integration script for some part of the process, then a second, then a third. At some point you will write an ugly ‘master’ script to keep your 17 import scripts in check and run them in just the right order. When you come back to the code later, you will have a hard time deciphering what you did and why, and what format the output data is in.\nPipelines to the rescue\n\nImplementing a proper data integration pipeline and a well-defined data model helps document your data flows and makes them traceable. More importantly, it simplifies the ETL development process, because it lets you easily re-run the whole process or parts of it. And you will have to modify and re-run your ETL, because your code changes, your output requirements change or the data changes.\n\nIn this talk I propose a setup for building maintainable data integration pipelines for everyday projects. This setup is embodied by ‘Ozelot’, my brand-new Python library for ETL. It is based on Luigi for pipeline management and SQLAlchemy as ORM layer. Ozelot gives you core functionality to quickly start building your own solution, including an ORM base class, database connection management and Luigi task classes that play nice with the ORM. It comes with extensively documented examples that walk you through various aspects of data integration.\n\nThe proposed setup works well for many small- to medium-sized projects -- projects, for which you previously might not have implemented a proper data integration pipeline. For big-data projects or those requiring live streaming data you probably want to consider alternative solutions.\nCore principles of data integration\n\nTaking one step back, I propose the following core principles for maintainable data integration:\n\n    Any and all data manipulation happens in the pipeline, in a single code base.\n    The pipeline represents all dependencies between data integration tasks.\n    Each task has a method for rolling back its operations.\n    Data is loaded into a single database, in a clearly defined, object-based data model that also encodes object relationships.\n    The whole process is fully automatic and thereby reproducible and traceable.\n\nI will discuss why I think that these principles are important, and how they are reflected in the proposed setup.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/D7fYa0NrCuE/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=D7fYa0NrCuE",
        "original_url": "https://www.youtube.com/watch?v=D7fYa0NrCuE"
    },
    {
        "index": 37,
        "title": "PyCon.DE 2017 Lauris Jullien - Verified fakes with OpenAPI",
        "url": "https://www.youtube.com/watch?v=NZqovz37Qcw",
        "video_id": "NZqovz37Qcw",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 436,
        "view_count": 155,
        "like_count": 1,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Lauris Jullien (@herrbot_DE)\n\nI am a backend engineer @Yelp in Hamburg for 3 years. Before that I was working with robots @Aldebaran Robotics. And before that I was doing my own startup :) I like clean APIs, all you can do with data, and as a good French, wine.\n\nAbstract\n\nTags: use-case python web\n\nIt can be hard to test code that depends on external services. Often such services are mocked, but with time, it can be challenging to keep these mocks up to date. Verified fakes can solve this problem, and we will see how to set them up using OpenAPI and python.\nDescription\n\nWhen your code depends on an external (third-party) service, testing it can be challenging. It may be expensive, slow or unreliable to call that service in tests. A classic approach is to mock such service or its network calls. But then the challenge becomes keeping the mocks up to date with the third-party services. Using verified fakes allows us to replace a service while guaranteeing that it stays up to date. This can be achieved in different ways: using recent network recordings (e.g. vcrpy), running a test suite against the mock and the real service to compare the results, or relying on an API contract like OpenAPI, or avro.\n\nOpenAPI* is a specification that documents how a service API works and enforces it by validating requests and responses for the service. We can then check the mocks against the OpenAPI specification to make sure they are up to date.\n\nIn this talk, I will explain how to set up such verified fakes with OpenAPI, and present a new open-source library, pyramid_mock_server. Here are the 3 main focuses:\n\n    Quick introduction on OpenAPI and how to use it with bravado, the python OpenAPI client\n    How to set up Verified Fakes: using pyramid_mock_server\n    Usage @Yelp: usage of fakes and OpenAPI for unit tests, acceptance tests, and documentation\n\nThe talk focuses on OpenAPI, but the learnings and methodology can be extended to any other API specification languages (or python web frameworks).\n\n* The OpenAPI initiative is a cross-vendor consortium focused on creating, evolving and promoting a vendor neutral description format for APIs. As an open governance structure under the Linux Foundation, its members include Google, IBM, Atlassian and PayPal.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/NZqovz37Qcw/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=NZqovz37Qcw",
        "original_url": "https://www.youtube.com/watch?v=NZqovz37Qcw"
    },
    {
        "index": 38,
        "title": "PyCon.DE 2017 Samuel Muñoz Hidalgo - The eye of the Python, an eye tracking system. From zero to...",
        "url": "https://www.youtube.com/watch?v=JckAPb-HpME",
        "video_id": "JckAPb-HpME",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 2394,
        "view_count": 135,
        "like_count": 0,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "The eye of the Python, an eye tracking system. From zero to... what eye learned.\n\nSamuel Muñoz Hidalgo | BEEVA\n\nI am Samuel (Samu for friends). With a curious mind I studied computer science, then focused in machine learning and IoT as a professional career. I haven't given up on my plan to take over the world; that's why my coworkers know a crazy idea is coming out, when I can't hide any longer a mischievous smile. I like to meet people and understand other points of view, and in return I like to show what I can do and teach what I have mastered. But, what drives me crazy is rollerskating with disco music.\n\nAbstract\n\nTags: data-science use-case python ai deep learning\n\nIs it possible to predict the point in the screen where a person is looking at? Easy to say but hard to do. An eye tracking system is the perfect project to learn the difficulties of applied machine learning. From gathering training data to building the final software with an acceptable performance.\nDescription\n\nIn this talk I will show how to build an eye tracking system from scratch.\n\nOnce the approach (machine learning) and the tools (Python ecosystem) are set, the important tasks are:\n\n    Making users addicted to a game built with Pygame in order to generate data.\n    Unleashing the power of deep learning over GPU with Tensorflow to train an Artificial Neural Network.\n    Exploiting the training model in real time so as to control a computer with the eyes with PyAutoGUI.\n\nThe path is full of pitfalls and every clear single step to the goal turns out to be a mountain of small but very important subtasks. Every iteration is a continuous struggle just to gain a bit of accuracy.\n\nDespite these efforts, at the very end we will see the difference between the theoretical and the real world. Our engineering skills will determine the usability of the new interface that allows us to move the mouse with our eyes; and we will learn that things don't need to be perfect, because humans are ...\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/JckAPb-HpME/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=JckAPb-HpME",
        "original_url": "https://www.youtube.com/watch?v=JckAPb-HpME"
    },
    {
        "index": 39,
        "title": "PyCon.DE 2017 Alexander Bauer - Large-scale machine learning pipelines using Luigi,...n",
        "url": "https://www.youtube.com/watch?v=VFB0rcfFCbg",
        "video_id": "VFB0rcfFCbg",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1948,
        "view_count": 535,
        "like_count": 4,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Large-scale machine learning pipelines using Luigi, PySpark and scikit-learn\n\nAlexander Bauer\n\nAlexander Bauer holds a Ph.D. in computer science. He has around 10 years industry experience, currently leading a team of data scientists at Lidl, one of the largest global discount supermarket chains. He is a Kaggle Master and regular speaker at the Frankfurt Predictive Analytics Meetup. He believes in agile software development practices and promotes Python as a primary language for data science applications in production.\n\nAbstract\n\nTags: data-science analytics python machine learning\n\nFor prescriptive analytics applications, data science teams need to design, build and maintain complex machine learning pipelines. In this talk, we demonstrate how such pipelines can be implemented in a robust, scalable and extensible manner using Python, Luigi, PySpark and scikit-learn.\nDescription\n\nData science teams working on real-world prescriptive analytics applications face the challenge to design, build and maintain considerably complex machine learning pipelines on a daily basis. Such pipelines include parsing data from multiple data sources, extracting relevant predictive features, executing training, validation, prediction steps and finally optimizing actions to meet desired business outcome so that they can be shared and visualized to business users. In this talk, we demonstrate how such pipelines can be implemented end-to- end in a robust, scalable and extensible manner using Python, Luigi, PySpark and scikit-learn. We will share our lessons learned from using this framework in a real-world demand forecasting use case.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/VFB0rcfFCbg/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=VFB0rcfFCbg",
        "original_url": "https://www.youtube.com/watch?v=VFB0rcfFCbg"
    },
    {
        "index": 40,
        "title": "PyCon.DE 2017 Nils Braun - Time series feature extraction with tsfresh - “get rich or die..",
        "url": "https://www.youtube.com/watch?v=Fm8zcOMJ-9E",
        "video_id": "Fm8zcOMJ-9E",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1631,
        "view_count": 16649,
        "like_count": 305,
        "comment_count": 24,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Time series feature extraction with tsfresh - “get rich or die overfitting”\n\nNils Braun (@_nilsbraun)\n\nCurrently I am doing my PhD in Particle Physics - which mainly involves development of software in a large collaboration. I love working with Python and C++ to process large amounts of data. Of course it needs to be processed as quickly as possible. I am working on the core reconstruction algorithms for our experiment, which are steered and controlled using Python. Apart from that, I was working as a Data Science Engineer for Blue Yonder, a leading machine learning company, where the idea for tsfresh was born. I am still heavily involved in the project. When I am not writing code, I am updating myself on the newest technical geek stuff (mostly cloud computing and deep learning) or play the guitar.\n\nAbstract\n\nTags: pydata time series data-science machine learning python ai\n\nHave you ever thought about developing a time series model to predict stock prices? Or do you consider log time series from the operation of cloud resources as being more compelling? In this case you really should consider using the time series feature extraction package tsfresh for your project.\nDescription\n\nTrends such as the Internet of Things (IoT), Industry 4.0, and precision medicine are driven by the availability of cheap sensors and advancing connectivity, which among others increases the availability of temporally annotated data. The resulting time series are the basis for manifold machine learning applications. Examples are the classification of hard drives into risk classes concerning specific defect, the log analysis of server farms for detecting intruders, or regression tasks like the prediction of the remaining lifespan of machinery. Tsfresh also allows to easily setup a machine learning pipeline that predicts stock prices, which we will demonstrate live during the presentation ;). The problem of extracting and selecting relevant features for classification or regression is these domains is especially hard to solve, if each label or regression target is associated with several time series and meta-information simultaneously – which is a common pattern in industrial applications. This talk introduces a distributed and parallel feature extraction and selection algorithm – the recently published Python library tsfresh. The fully automated extraction and importance selection does not only allow to reach better machine learning classification scores, but in combination with the speed of the package, also allows to incorporate tsfresh into automated AI-pipelines.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/Fm8zcOMJ-9E/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=Fm8zcOMJ-9E",
        "original_url": "https://www.youtube.com/watch?v=Fm8zcOMJ-9E"
    },
    {
        "index": 41,
        "title": "PyCon.DE 2017 Adrian Seyboldt - An introduction to PyMC3",
        "url": "https://www.youtube.com/watch?v=FKhivuCLIT0",
        "video_id": "FKhivuCLIT0",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 2612,
        "view_count": 821,
        "like_count": 6,
        "comment_count": 2,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Adrian Seyboldt\n\nI studied Mathematics and Bioinformatics in Bonn and Tübingen and I am a core developer of pymc3 since Feb 2017. Currently, I work for Quantopian on the development of Bayesian Methods for portfolio allocation.\n\nAbstract\n\nTags: python data-science machine learning analysis\n\nPyMC3 allows you to build statistical models for a wide range of datasets, use those models to estimate underlying parameters, and compute the uncertainty about those parameters. In this talk I will try to give a gentle introduction to PyMC3, and help avoid common pitfalls for new users.\nDescription\n\nSome of the problems that are discussed in the context of the reproducibility crisis in science and statistics can be solved or alleviated by tools like PyMC3 or Stan. They allow users to build much more realistic models and get a full distribution of the possible values for parameters as output – instead of p-values that are often hard to interpret correctly. Thanks to Hamiltonian and Variational methods, they are more flexible and can be applied to larger problems than predecessors like JAGS and BUGS. However, these new methods also come with challenges. Writing good models isn't easy, and when inference algorithms cry out in pain, they need someone who listens to them. This talk uses some real-world applications to give an introduction to PyMC3, without requiring a lot of background in math, statistics or programming.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/FKhivuCLIT0/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=FKhivuCLIT0",
        "original_url": "https://www.youtube.com/watch?v=FKhivuCLIT0"
    },
    {
        "index": 42,
        "title": "PyCon.DE 2017 Yenny Cheung - Technical Lessons Learned from Pythonic Refactoring",
        "url": "https://www.youtube.com/watch?v=Yq9-b2JKUyU",
        "video_id": "Yq9-b2JKUyU",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1863,
        "view_count": 2152,
        "like_count": 42,
        "comment_count": 2,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Yenny Cheung (@yennycheung)\n\nYenny Cheung, Software Engineer, Biz National, Yelp\n\nOriginally from Hong Kong, Yenny moved to Pennsylvania in the US to study Computer Science at Swarthmore College. She attended the Grace Hopper Conference three times during her college years. After graduation, Yenny moved to Hamburg and joined Yelp as a full-stack software engineer. Yenny works on the Biz National team, where she is scaling advertising tools and reporting for multi-location businesses and franchises. She is a pioneer of the Awesome Women in Engineering Group at Yelp in Germany and she is active in Women in Engineering Meetups in Hamburg. Outside of work, Yenny enjoys yelping for good food and painting.\n\nAbstract\n\nTags: code smell api refactoring python web\n\nEver stumbled upon poorly-maintained codebases that suck away your productivity? Fear no more! This talk addresses how to identify code smell (from Brie to Bleu cheese) and go through examples to refactor code and APIs. You will learn the art of writing clean, maintainable and idiomatic Python code.\nDescription\n\nThis talk targets beginner software engineers yet also aims to provide interesting content for more advanced developers. You will leave knowing more about code smells and understanding the whys and hows of refactoring.\n\nCode smells refer to the symptoms of problematic code design. Identifying different types of code smells is the first step to successful refactoring. I will talk through some classic examples:\n\n    Unnecessarily complex code\n    Inappropriate naming\n    Duplicated code\n    Non-modularized code\n\nKnowing what to refactor, I will share a few learnings that lead to good quality code:\n\n    Good python practices (The Zen of Python & PEP8)\n    Good use of data structures (named tuples & sets)\n    The art of pythonic naming\n    DRY principle\n    Separation of concerns principle (modularity & encapsulation)\n\nRefactoring also makes testing easier. I will discuss how to write unit tests that cover all use cases. Finally I will talk about integration testing strategies that ensure that you and people who want to modify your API in the future are not breaking upstream code.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/Yq9-b2JKUyU/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=Yq9-b2JKUyU",
        "original_url": "https://www.youtube.com/watch?v=Yq9-b2JKUyU"
    },
    {
        "index": 43,
        "title": "PyCon.DE 2017 Patrick Schemitz - From Java to Python: Migrating Search Functionality at billiger.de",
        "url": "https://www.youtube.com/watch?v=2fuW9ITUXrU",
        "video_id": "2fuW9ITUXrU",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 1758,
        "view_count": 592,
        "like_count": 6,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Patrick Schemitz\n\nPatrick is a Senior Scientist at solute GmbH. An avid Pythonista since 2003, his main responsibility is the billiger.de search functionality, which he (co-) wrote using first Lucene, later Solr and now SolrCloud. Besides that, he wrote the SVM-based offer categorization at billiger.de and has a keen interest in machine learning. Patrick holds a Ph.D. in particle physics from Karlsruhe university.\n\nAbstract\n\nTags: solrcloud solr search python\n\nbilliger.de is a German price comparison site. Search is handled by a heavily customized Solr setup. When switching to SolrCloud earlier this year, instead of porting our custom SolrComponents to SolrCloud, we ended up re-implementing them in a Python service layer. Here we show how, and why.\nDescription\n\nThe search on our price comparison site billiger.de is implemented using Solr and half a dozen custom SolrComponents. When switching from Solr to SolrCloud earlier this year, we had to go over all our custom components in order to make them cluster-ready. What we ended up doing instead was re-implementing the custom functionality in a Python service layer that in turn uses stock SolrCloud. This talk describes our journey, shows some code and advocates hiding implementation details like Solr v. SolrCloud behind a service layer. Ported functionality includes boosting more successful documents, identifying brands and categories in queries, \"minimum match\" search and facet ranking and alternatives.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/2fuW9ITUXrU/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=2fuW9ITUXrU",
        "original_url": "https://www.youtube.com/watch?v=2fuW9ITUXrU"
    },
    {
        "index": 44,
        "title": "PyCon.DE 2017 Nafiul Islam - Graphql in the Python World",
        "url": "https://www.youtube.com/watch?v=FpQpF0BTrJU",
        "video_id": "FpQpF0BTrJU",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171208",
        "duration": 1503,
        "view_count": 354,
        "like_count": 4,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Nafiul Islam (@gamesbrainiac)\n\nI'm Nafiul. I am a Bangladeshi who got lucky and is now working in Amsterdam. I have a deep passion for teaching and learning; I spend a lot of time on StackOverflow, answering questions, and I also spend a lot of time mentoring people on hackhands. I've even written a book in PyCharm, to share all of my IDE secrets. I don't I have a right to have people listening to me, but I feel that what I have to say is worth a try listening to, and I'm also (I believe) a good guy to have a discussion with.\n\nI'm not a fan of One Direction, primarily because they seem like a bunch of overgrown teenage boys unable to come to terms with their own maturity. In case you want to hear some real music you might want to try out \"Fortunate Son\" by Creedence Clearwater Revival. I have more :)\n\nAbstract\n\nTags: web python\n\nSo you've heard about this new thing called Graphql. What is it all about? What problems does it solve, and most importantly, how can you leverage it the python ecosystem? This talk is a tell all on what what Graphql is and how you can start using it with Python.\nDescription\n\nGraphql is a new way of communicating with your data sources. It will very likely replace RESTful APIs because it provides a better framework for dealing with your data. This talk is about what Graphql is, what problems it actually solves in comparison to RESTful APIs, and if you like it, how you can get started using the python graphene library, and how to use it with different ORMs like Django and SQLAlchemy.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/FpQpF0BTrJU/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=FpQpF0BTrJU",
        "original_url": "https://www.youtube.com/watch?v=FpQpF0BTrJU"
    },
    {
        "index": 45,
        "title": "PyCon.DE 2017 Stephan Erb - The Snake in the Tar Pit: Complex Systems with Python",
        "url": "https://www.youtube.com/watch?v=pee-e01DiyI",
        "video_id": "pee-e01DiyI",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171208",
        "duration": 2471,
        "view_count": 334,
        "like_count": 2,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Stephan Erb (@ErbStephan)\n\nStephan Erb is a software engineer driven by the goal to make Blue Yonder's data scientists more productive. Stephan holds a master's degree in computer science from the Karlsruhe Institute of Technology (KIT). He is a PMC member of the Apache Aurora project and tweets at @ErbStephan.\n\nAbstract\n\nTags: system design complexity python\n\nThe Zen of Python motivates us to build software that is easy to maintain and extend. In reality however, we often end up with systems that are quite the opposite: complex and hard to change. In this talk, we will have a look at why this happens and how we can try to prevent it.\nDescription\n\nBeautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. ...\n\nThat is how the Zen of Python motivates us Python programmers to build software that is easy to maintain and extend. In reality however, after years of development, we often end up with systems that are quite the opposite: large code bases, slow turn around times, and brittle operations.\n\nIn this talk we will explore software design methodologies that can help to prevent this from happening. We will have a closer look at seminal work by computer scientists that has proven to be useful in practice. This includes:\n\n    Out of the Tar Pit by Ben Moseley and Peter Marks\n    End-to-End Arguments in System Design by Saltzer et al.\n    Programming Pearls by Jon Bentley\n\nA conference talk is clearly not sufficient to cover the breath of the mentioned literature. Main goal of the talk is therefore to illustrate a few key messages using examples from real-world Python projects.\n\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi/pee-e01DiyI/hqdefault.jpg",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=pee-e01DiyI",
        "original_url": "https://www.youtube.com/watch?v=pee-e01DiyI"
    },
    {
        "index": 46,
        "title": "PyCon.DE 2017 Ami Tavory - Getting Scikit-Learn To Run On Top Of Pandas",
        "url": "https://www.youtube.com/watch?v=boXOVvu43ZI",
        "video_id": "boXOVvu43ZI",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171208",
        "duration": 1738,
        "view_count": 164,
        "like_count": 1,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Ami Tavory\n\nAmi is a data scientist at Facebook Research's Core Data Science group. He previously worked as a machine learning researcher in the fields of bioinformatics and algorithmic trading. In 2010 he received a Ph.D in Electrical Engineering from Tel Aviv University, in the field of financial information theory. His bachelor's and master's are from Tel Aviv University too.\n\nAmi uses Python and C++ for data analysis. He contributed to various open source projects, and is the author of a libstd C++ extension shipped with g++ (pb_ds: policy-based data structures).\n\nAbstract\n\nTags: code-introspection scikit-learn pandas data-science python machine learning\n\nScikit-Learn is built directly over numpy, Python's numerical array library. Pandas adds to numpy metadata and higher-level munging capabilities. This talk describes how to intelligently auto-wrap Scikit-Learn for creating a version that can leverage pandas's added features.\nDescription\n\nScikit-Learn is the de-facto standard Python library for general-purpose machine learning. It operates over NumPy, an efficient, but low-level, homogeneic array library. Pandas adds to NumPy metadata, heterogeneity, and higher-leve munging capabilities.\n\nIn the field of visualization, newer generation libraries, e.g., Seaborn and Bokeh, are providing safer, more readable, and higher-level functionality, by operating over Pandas data structures. Some of these are implemented using Matplotlib, a lower-level NumPy-based plotting library.\n\nThis talk describes a library for a Pandas-based version of sickit-learn. Here, too, giving a Pandas interface to a machine-learning library, provides code which is safer to use, more readable, and allows direct integration with Pandas's higher-level munging capabilities.\n\nDue to the large-scale, and evolving nature, of sicikit-learn's codebase, it is infeasible to manually wrap it. Except for a small number of intentional deviations from sickit-learn, the library wraps Scikit-Learn modules lazily through module and class introspection, and dynamic module loading.\n\nFollowing a short review of the relevant points of Pandas and Scikit-Learn, the talk is roughly divided into two aspects:\n\n    Scikit-Learn And Pandas User Perspective\n        Safety Advantages Of Pandas-Based Estimators\n        Using Metadata For Inter-Instance Aggregated Features And Cross-Validation\n        Using Metadata For Advanced Meta-Algorithms: Stacking, Nested Labeled And Stratified Cross-Valdiation\n    Python Develop Perspective\n        Unique Challenges Of Scikit-Learn Introspection And Decoration\n        Two Approaches For Wrapping Scikit-Learn Estimators\n        Lazy Dynamic Module Loading\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/boXOVvu43ZI/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=boXOVvu43ZI",
        "original_url": "https://www.youtube.com/watch?v=boXOVvu43ZI"
    },
    {
        "index": 47,
        "title": "PyCon.DE 2017 Yasir Khan - Data Science Best Practices : From Proof of Concepts to Production",
        "url": "https://www.youtube.com/watch?v=OPw0VrZdLdo",
        "video_id": "OPw0VrZdLdo",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171208",
        "duration": 1713,
        "view_count": 966,
        "like_count": 7,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Yasir Khan\n\nYasir Khan is working as a Data Science Manager. He has nearly 12 years of experience in consulting and R&D domains. He obtained his PhD in Applied Machine Learning and has been invited as a speaker at several leading international forums and conferences. He has nearly 12 research articles and journals to his credit and book chapters published by Cambridge University Press.\n\nIn his spare time he loves flying WWII aircrafts at a local aeroclub and is an avid scuba diver. He is also an investor in 2 technology startups.\n\nAbstract\n\nTags: use-case analytics python machine learning ai business\n\nThis presentation will benefit the audience as it brings forward the practical issues in the industry today as we move towards industrializing data science algorithms. We will discuss the best practices around organization, methodology and tools to integrate a data science project into production.\nDescription\n\nAs the industry understands the importance of Data Science for transforming businesses an interesting trend is arising. We have started seeing a widening gap & increasing difficulty to move teams from Proof of Concepts to Production. Apart from searching for the best data scientists, the industry is now looking for answers to organize & federate the data teams around practical business use cases. In this talk, Yasir Khan will provide an overview of the bottlenecks, and hurdles involved in a practical data project. While citing lessons learned from industry, this presentation will focus on important aspects such as business centric, data culture, data pipelines, organization, methodology & tools all of which are important but seldom ignored in large corporations.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/OPw0VrZdLdo/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=OPw0VrZdLdo",
        "original_url": "https://www.youtube.com/watch?v=OPw0VrZdLdo"
    },
    {
        "index": 48,
        "title": "PyCon.DE 2017 David Dao - Really Deep Neural Networks with PyTorch",
        "url": "https://www.youtube.com/watch?v=ZUHhNuw9Tlc",
        "video_id": "ZUHhNuw9Tlc",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171208",
        "duration": 2438,
        "view_count": 908,
        "like_count": 13,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "David Dao (@dwddao)\n\nDavid is a PhD student at ETH Zurich, working on Deep Reinforcement Learning. Before joining ETH Zurich, he was an autonomous driving researcher at Mercedes-Benz Research in Silicon Valley and a graduate student at the Broad Institute of MIT and Harvard.\n\nDavid is a firm believer in open source and is organising Germany's largest deep learning meetup series, and Silicon Valley's self-driving AI series. He is a contributor to popular machine intelligence frameworks such as TensorFlow and PyTorch and speaks chinese with swabian accent.\n\nAbstract\n\nTags: deep learning ai machine learning python autonomous-driving pytorch\n\nModern neural networks have hundreds of layers! How can we train such deep networks? Simply stacking layers on top doesn't work! This talk introduces the deep learning library PyTorch by explaining the exciting math, cool ideas and simple code behind what makes really deep neural networks work.\nDescription\n\nModern neural networks consist of hundreds of computation layers! These very deep architectures consistently outperform shallower networks in a variety of tasks. However just simply stacking layers on top of each other won't work because the gradients are either vanishing or exploding during optimisation procedure. This talk explains the exciting math, cool ideas and elegant code that modern neural network architectures such as ResNets, HighwayNets and DenseNets are applying to circumvent the problem using PyTorch. PyTorch is a relatively new deep learning framework that is deeply integrated into Python. Unlike other frameworks such as TensorFlow and Theano, it uses tape-based automatic differentiation to run computation immediately, supports dynamic neural networks and provides a powerful GPU-accelerated Tensor library. The talk concludes with some real-world use-cases for very deep neural networks in chemical-genetic profiling and autonomous driving.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/ZUHhNuw9Tlc/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=ZUHhNuw9Tlc",
        "original_url": "https://www.youtube.com/watch?v=ZUHhNuw9Tlc"
    },
    {
        "index": 49,
        "title": "PyCon.DE 2017 Stephan Jeansch - Migrating existing codebases to using type annotations",
        "url": "https://www.youtube.com/watch?v=JKvtrb2GWMY",
        "video_id": "JKvtrb2GWMY",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171201",
        "duration": 2252,
        "view_count": 104,
        "like_count": 0,
        "comment_count": null,
        "tags": [
            "PyCon.DE2017",
            "Python",
            "PyCon.DE",
            "PyData"
        ],
        "categories": [
            "Education"
        ],
        "description": "Stephan Jaensch (@s_jaensch)\n\nI'm a backend developer and tech lead at Yelp, working on the core infrastructure of the Commerce group. I've worked and continue to work on our Business Owner App backend and am deeply involved in the service infrastructure at Yelp. My main focus area is inter-service development. I'm one of the maintainers of the Python stack for Swagger / OpenAPI (bravado etc). I also work on developer productivity, such as making integration and end-to-end testing faster and more reliable.\n\nAbstract\n\nTags: python\n\nYou have an existing codebase of tens or hundreds of thousands of lines of Python code? Learn how to get started with type annotations! Get your teammates (and yourself!) to always annotate your code. Find out what unexpected issues you might run into and how to solve them, all with this talk.\nDescription\n\nYou've heard about type annotations, you know they help reduce bugs and improve documentation especially for large codebases, and you've attended an introductory talk or read a tutorial about using them. But how do you get started using them with your big, existing codebase? How do you make sure your colleagues will be annotating new code they write - or existing code they're changing? And how do you get around some of the issues you might run into when using the still-beta type checker mypy on your codebase?\n\nThis talk will start where the typical introductory Python type annotation talks end and discuss the real-world challenges when starting to annotate types with an existing codebase of tens or hundreds of thousands of lines of code. I'll walk you through best practices learned from doing just that at Yelp, telling you about some of the roadblocks we hit (and how we got past them). We'll also take a look at:\n\n    how you can get the most out of type annotations even with non-annotated third-party libraries\n    how to deal with decorators and other things that currently don't work well with annotations\n    when the only way to get proper type checking is through refactoring your code.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de\nVideo editing: Sebastian Neubauer & Andrei Dan\nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/JKvtrb2GWMY/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=JKvtrb2GWMY",
        "original_url": "https://www.youtube.com/watch?v=JKvtrb2GWMY"
    },
    {
        "index": 50,
        "title": "PyCon.DE 2017 Patrick Mühlbauer - Observing your applications with Sentry and Prometheus",
        "url": "https://www.youtube.com/watch?v=f3WO4bpLySs",
        "video_id": "f3WO4bpLySs",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171222",
        "duration": 1373,
        "view_count": 393,
        "like_count": 4,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Patrick Mühlbauer\n\nPatrick's a Software Engineer from Karlsruhe, working in Blue Yonder's Platform team. He likes this devops thing and enjoys instrumenting code to collect metrics and create nice and shiny Grafana Dashboards. To look at them. For hours. He also likes beer. A lot. Let's have some at the Social Event!\n\nAbstract\n\nTags: web monitoring observability\n\nIf you have services running in production, something will fail sooner or later. We cannot avoid this completely, but we can prepare for it. In this talk we will have a look at how Sentry and Prometheus can help to get better insight into our systems to quickly track down the cause of failure.\nDescription\n\nWhen your services start to behave in a strange way, for example due to bugs introduced in the newly deployed release, you want get informed about that as fast as possible. Preferably by your own monitoring and not by one of your customers. We will have a look at how Sentry and Prometheus can help with that.\n\nSentry is a real-time error tracking system, which can notify you when exceptions in your application occur. Additionally, it provides lots of context so that crashes can be reproduced and fixed very quickly.\n\nPrometheus is a systems and service monitoring system, collecting metrics from all kinds of targets. The collected metrics can help to get insight in what's actually going on in your services.\n\nRough outline for the talk:\n\n    What's observability and why should I care?\n    Erros and Exceptions in Python\n    How to track errors with Sentry\n    Introduction to Metrics and Prometheus\n    Instrumenting Python applications\n    Summary\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/f3WO4bpLySs/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=f3WO4bpLySs",
        "original_url": "https://www.youtube.com/watch?v=f3WO4bpLySs"
    },
    {
        "index": 51,
        "title": "PyCon.DE 2017 Christian Theune - An Admin's Cornucopia - Python Is More Than Just A Better Bash",
        "url": "https://www.youtube.com/watch?v=CQ3qwmld5V8",
        "video_id": "CQ3qwmld5V8",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171222",
        "duration": 1782,
        "view_count": 181,
        "like_count": 0,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Christian Theune (@theuni)\n\nAbstract\n\nTags: python devops\n\nPython's versatility is known to admins - in this talk I'd like to show how it fits for many small and big challenges I meet regularly: from tiny scripts to large systems. Also, I'll show how using the languages' advanced and/or newer features makes scripts more compact and robust.\nDescription\n\nDRAFT Unfortunately I got sick just a few days before the CFP date, so hopefully you can live with a draft description for now. I'll be happy to update this during or after the review as needed.\n\nI'd like to give a hands-on approach for how to use Python in a mixed environment where you may be writing very small standalone scripts, small integration programs and/or larger systems. As an admin bash is always a very close friend and one quickly uses it and then ends up with large scripts that should never have seen the day of light. Also, admins nowadays might be writing larger software bases that are actual software projects and the lines between \"ops\" and \"devs\" are - on purpose - more and more blurry. However, there are some differences in \"application development\" and \"system development\". Those are more of a mindset issue, however, and Python as a technology fits in both situations quite well and allows one to transition between those modes rather seamlessly.\n\nI'll be reviewing specific pieces of code from our infrastructure, ranging from smaller scripts to subsystems to a full-born Pyramid application that we use for inventory management. All of those will use different versions of Python, different libraries and different approaches how to solve certain problems. I will likely show how not to do things as well.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/CQ3qwmld5V8/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=CQ3qwmld5V8",
        "original_url": "https://www.youtube.com/watch?v=CQ3qwmld5V8"
    },
    {
        "index": 52,
        "title": "PyCon.DE 2017 Dominik Benz -  Flow is in the Air: Best Practices with Apache Airflow",
        "url": "https://www.youtube.com/watch?v=Ea3smcbnGxE",
        "video_id": "Ea3smcbnGxE",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171222",
        "duration": 2865,
        "view_count": 532,
        "like_count": 2,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Flow is in the Air: Best Practices of Building Analytical Data Pipelines with Apache Airflow\n\nDominik Benz (@john_maverick)\n\nDominik Benz holds a PhD from the University of Kassel in the field of Data Mining on the Social Web. Since 2012 he is working as a Big Data Engineer at Inovex GmbH. In this time, he was involved in several projects concerned with establishing analytical data platforms in various companies. He is most experienced in tools around the Hadoop Ecosystem like Apache Hive and Spark, and has hands-on experience with productionizing analytical applications.\n\nAbstract\n\nTags: workflow data pipeline data-science analytics\n\nApache Airflow is an Open-Source python project which facilitates an intuitive programmatic definition of analytical data pipelines. Based on 2+ years of productive experience, we summarize its core concepts, detail on lessons learned and set it in context with the Big Data Analytics Ecosystem.\nDescription\nMotivation & Outline\n\nCreating, orchestrating and running multiple data processing or analysis steps may cover a substantial portion of a Data Engineer and Data Scientist business. A widely adopted notion for this process is a \"data pipeline\" - which consists mainly of a set of \"operators\" which perform a particular action on data, with the possibility to specify dependencies among those. Real-Life examples may include:\n\n    Importing several files with different formats into a Hadoop platform, perform data cleansing, and training a machine learning model on the result\n    perform feature extraction on a given dataset, apply an existing deep learning model to it, and write the results in the backend of a microservice\n\nApache Airflow is an open-source Python project developed by AirBnB which facilitates the programmatic definition of such pipelines. Features which differentiate Airflow from similar projects like Apache Oozie, Luigi or Azkaban include (i) its pluggable architecture with several extension points (ii) the programmatic approach of \"workflow is code\" and (iii) its tight relationship with the the Python as well as the Big Data Analytics Ecosystem. Based on several years of productive usage, we briefly summarize the core concepts of Airflow, and detail in-depth on lessons learned and best practices from our experience. These include hints for getting efficient quickly with Airflow, approaches to structure workflows, integrating it in an enterprise landscape, writing plugins and extentions, and maintaining it in productive environment. We conclude with a comparison with other analytical workflow engines and summarize why we have chosen Airflow.\nQuestions answered by this talk\n\n    What are the core concepts of Apache Airflow?\n    How can Airflow help me with moving data pipelines from analytics to production?\n    Which concepts of Airflow make it more slim and more efficient compared to Apache Oozie?\n    How can I specify dynamic dependencies at runtime between my analytical data processing steps?\n    Which facilities does Airflow offer to enable automation and orchestration of analytical tasks?\n    How can I extend the built-in facilities of Airflow by writing Python plugins?\n\nPeople who benefit most from this talk\n\n    Data Scientists who are looking for a slim library to automate and control their data processing steps\n    Data Engineers who want to save time debugging static workflow definitions (e.g. in XML)\n    Project leaders interested in tools which lower the burden of moving from analytics to production\n    Hadoop Cluster administrators eager to save cluster resources\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi/Ea3smcbnGxE/hqdefault.jpg",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=Ea3smcbnGxE",
        "original_url": "https://www.youtube.com/watch?v=Ea3smcbnGxE"
    },
    {
        "index": 53,
        "title": "PyCon.DE 2017 Christine Spindler - Python on bare metal  Beginners tutorial with MicroPython",
        "url": "https://www.youtube.com/watch?v=MpRXnyFeEwg",
        "video_id": "MpRXnyFeEwg",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171222",
        "duration": 2424,
        "view_count": 1176,
        "like_count": 15,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Christine Spindler\n\nChristine holds a degree in economics and electrical engineering. Since the beginning of 2016 she works with Damien P. George and takes care of logistics and production at George Robotics Ltd. – the company behind MicroPython.\n\nAbstract\n\nTags: python\n\nMicroPython is a complete reimplementation of Python that runs on small devices like microcontrollers. In this hands-on workshop I'll show how easy it is to use MicroPython on a pyboard.\nDescription\n\nEverybody that attends will get a MicroPython pyboards to follow all examples on their own machine. Bring a laptop and Micro USB cable along – make sure it has a data line.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi_webp/MpRXnyFeEwg/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=MpRXnyFeEwg",
        "original_url": "https://www.youtube.com/watch?v=MpRXnyFeEwg"
    },
    {
        "index": 54,
        "title": "PyCon.DE 2017 Florian Rhiem - Integrating Jupyter Notebooks into your Infrastructure",
        "url": "https://www.youtube.com/watch?v=xplmuHEFqCg",
        "video_id": "xplmuHEFqCg",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171222",
        "duration": 1969,
        "view_count": 381,
        "like_count": 0,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Florian Rhiem (@FlorianRhiem)\n\nFlorian Rhiem is a scientific software developer at Forschungszentrum Jülich in the Scientific IT-Systems department of Peter Grünberg Institute / Jülich Centre for Neutron Science. In 2014, he finished his master's thesis on visualization of multidimensional functions and graduated as Master of Science in Technomathematics. He works with Python and C, mostly focusing on 3D visualization software.\n\nAbstract\n\nTags: jupyterhub jupyter use-case\n\nJupyter Notebooks combine executable code and rich text elements in a web application. In this talk you will learn how a custom JupyterHub installation can be used to integrate Jupyter Notebooks into your infrastructure, including existing authentication methods and custom software distributions.\nDescription\n\nJupyter Notebooks allow you to create and share interactive documents containing both rich text and executable code. As researchers, you can write Notebooks that allow your readers to reproduce your results on their own. As data scientists, you can use interactive visualizations to explore and analyze data sets directly in your browser. With official support for Python, Julia and R and community support for programming languages ranging from Fortran to JavaScript, Jupyter can be used in a wide variety of workflows.\n\nIn this talk, you will learn how a custom JupyterHub installation has been used to seamlessly integrate Jupyter Notebooks into the existing infrastructure at the Jülich Centre for Neutron Science and the Peter Grünberg Institute. After introducing JupyterHub and its components, the presentation will show how each can be customized to use already existing resources and services, such as LDAP authentication, NFS-based storage and custom software distributions.\n\nRecorded at PyCon.DE 2017 Karlsruhe: pycon.de \nVideo editing: Sebastian Neubauer & Andrei Dan \nTools: Blender, Avidemux & Sonic Pi",
        "thumbnail": "https://i.ytimg.com/vi/xplmuHEFqCg/hqdefault.jpg",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=xplmuHEFqCg",
        "original_url": "https://www.youtube.com/watch?v=xplmuHEFqCg"
    },
    {
        "index": 55,
        "title": "PYCON.DE 2017 Ansgar Schmidt - Python with Apache OpenWhisk",
        "url": "https://www.youtube.com/watch?v=E9Yj4g9LuJc",
        "video_id": "E9Yj4g9LuJc",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171229",
        "duration": 1805,
        "view_count": 320,
        "like_count": 6,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Ansgar Schmidt (@ansi)\n\nI am a full time nerd. Really thrilled by technology and love to make and hack. From PCB design, soldering, embedded development to server- backend hacking. Beside way to many small projects I work on a mobile robot based on ROS and try to add human interaction to it.\n\nAbstract\n\nTags: devops use-case python web business\n\nOpenWhisk is an opensource implementation of a so called serverless computing platform. At a live presentation I will show how to write an serverless application and how to deal with libraries and events. OpenWhisk is an open source alternative to AWS lambda or MS functions.\nDescription\n\nHi, I am a developer advocate working for IBM. Meaning I am a technician and geek not a salesman. In my talk I will give a live demo on how to use the open source implementation of a serverless compute platform called OpenWhisk and help python developers on how to use it and write serverless applications.",
        "thumbnail": "https://i.ytimg.com/vi_webp/E9Yj4g9LuJc/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=E9Yj4g9LuJc",
        "original_url": "https://www.youtube.com/watch?v=E9Yj4g9LuJc"
    },
    {
        "index": 56,
        "title": "PYCON.DE 2017 David Wölfe - From 0 to Continuous Delivery in 30 minutes.",
        "url": "https://www.youtube.com/watch?v=yqpOrB0JDto",
        "video_id": "yqpOrB0JDto",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171229",
        "duration": 1851,
        "view_count": 250,
        "like_count": 1,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "David Wölfle\n\nPlease find information to my person on https://www.linkedin.com/in/david-woelfle/\n\nAbstract\n\nTags: hands on continuous delivery devops python gitlab conda\n\nAn introduction and hands on example how to start Continuous Delivery for python (or whatever) projects with conda and gitlab, which are open source, free to use, and if you wish even available as a cloud service.\nDescription\n\nSo, you wrote this neat software, which uses one or two cool 3rd party packages and now you have to get it running on your test or prod servers. Now you could check out your code and install the dependencies manually but that seems like a lot of wasted time, right? Shouldn't there be a more convenient way of doing this?\n\nYes! And it's easier than you think! In this talk you'll learn how to build a Continuous Delivery pipeline for your python (or whatever) projects in 30 Minutes.\n\nIn the beginning you'll get a quick introduction into the topic and the used tools, which are open source and free to use. For those who know a little about the subject: We will use conda (https://conda.io) to build the software package and for the dependency handling. GitLab.com will process the actual pipeline for us and Anaconda.com serves us with a package distribution channel.\n\nAfter that it's \"hands on\" time. We will go step by step trough the process. Starting with the creation of a little dummy program. Then extending the dummy in such manner that it can be build into a package. Afterwards we will create the pipeline and finally do a demo deploy. And if the gods of conference internet are merciful, you see all this as a live demonstration.",
        "thumbnail": "https://i.ytimg.com/vi_webp/yqpOrB0JDto/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=yqpOrB0JDto",
        "original_url": "https://www.youtube.com/watch?v=yqpOrB0JDto"
    },
    {
        "index": 57,
        "title": "PYCON.DE 2017 Dave Halter - Python is Weird",
        "url": "https://www.youtube.com/watch?v=zz9yLOXo2Qk",
        "video_id": "zz9yLOXo2Qk",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171229",
        "duration": 1594,
        "view_count": 625,
        "like_count": 5,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Dave Halter (@jedidjah_ch)\n\nI'm an engineer for cloudscale.ch. We are serving cloud services (mostly IaaS) with a focus on simplicity. In my free time I work a lot on parsers/type inference for the Python world. Jedi is probably the project people would know me for.\n\nAbstract\n\nTags: tokenizer parser python\n\nA lot of people think that Python is a really simple and straightforward language. Python hides a lot of peculiarities very well, but for the sake of this talk we will try to uncover them.\n\nIs ++4; valid Python? And what does it do? Let me give you an introduction into tokenizers/parsers.\nDescription\n\nA lot of people think that Python is a really simple and straightforward language. Python hides a lot of peculiarities very well, but for the sake of this talk we will try to uncover them.\n\nI will be explaining how the whole process of tokenizing - parsing - ast creation - bytecode works and will use odd Python code to give you an insight on the internals. Do you think ++4; is valid Python? Or how about 0jif.1else-2? There are no spaces in it. Go figure! \"Edge cases\" will help us understand the inner workings of Python. These two cases are possible, because of the tokenizer. Other \"weird\" Python code is waiting as soon as you start looking at the grammar file. Have you for example heard about lambda generators?\n\nWe will be looking into how modules, classes and instances are really just fancy dictionaries and how importing is really nothing else than storing a module into a dictionary (sys.modules).\n\nThere are a lot of things we can learn from diving deep into the details of our beloved languages. This talk will give you a very small introduction in how languages are built and explain how Python itself is defined by its parser, tokenizer and bytecode generation. Knowing how those abstractions work makes you a better Python programmer, because you will know better how the language behaves.",
        "thumbnail": "https://i.ytimg.com/vi_webp/zz9yLOXo2Qk/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=zz9yLOXo2Qk",
        "original_url": "https://www.youtube.com/watch?v=zz9yLOXo2Qk"
    },
    {
        "index": 58,
        "title": "PYCON.DE 2017 ChristophHeer - Automated testing with 400TB memory",
        "url": "https://www.youtube.com/watch?v=Lro5wC_HxhE",
        "video_id": "Lro5wC_HxhE",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20171229",
        "duration": 1959,
        "view_count": 234,
        "like_count": 2,
        "comment_count": null,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Christoph Heer (@ChristophHeer)\n\nI’m an Infrastructure Engineer in the team behind SAP’s huge test infrastructure for SAP HANA. In my spare time, I develop web applications with Django or playing around with new programming languages like Rust.\n\nAbstract\n\nTags: python use-case devops business\n\nSAP operates a dedicated test infrastructure with more than 400TB main memory for its in-memory database SAP HANA. All custom implementations like improved scheduling, caching of artifacts and monitoring were implemented in our favorite programming language Python.\nDescription\n\nSAP operates a large test infrastructure to test its in-memory database SAP HANA. In 2010, we started with a single Jenkins master with ten nodes. But to keep our testing time acceptable for the growing number of developers we had to scale up, which led to multiple different scaling challenges. The current test infrastructure is powered by more than thousand physical servers and provides different services like continuous integration, code coverage and code linting for a huge C++ project. These services are essential for developing and shipping new SAP HANA versions.\n\nThis talk provides insights into how we scaled and improved our test infrastructure. All custom implementations like improved scheduling, expressive test configuration and caching of artifacts were implemented in our favorite programming language Python. With the flexibility and power of Python it has been easier to extend, optimize and adapt the infrastructure for new requirements like different CPU architectures and newer operating system versions.",
        "thumbnail": "https://i.ytimg.com/vi_webp/Lro5wC_HxhE/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=Lro5wC_HxhE",
        "original_url": "https://www.youtube.com/watch?v=Lro5wC_HxhE"
    },
    {
        "index": 59,
        "title": "PYCON.DE 2017 Thomas Reifenberg - Project Avatar - Telepresence robotics with Nao and Kinect",
        "url": "https://www.youtube.com/watch?v=SGpdc-9QAWE",
        "video_id": "SGpdc-9QAWE",
        "uploader": "PyCon DE",
        "uploader_id": "@PyConDE",
        "uploader_url": "https://www.youtube.com/@PyConDE",
        "upload_date": "20180111",
        "duration": 2252,
        "view_count": 200,
        "like_count": 2,
        "comment_count": 1,
        "tags": [],
        "categories": [
            "Education"
        ],
        "description": "Thomas Reifenberger,Martin Foertsch (@reifenbt)\n\nMartin Förtsch is an IT-consultant of TNG Technology Consulting GmbH (https://tngtech.com) based in Unterföhring near Munich who studied computer sciences. Workwise his focus areas are Agile Development (mainly) in Java, Search Engine Technologies, Information Retrieval and Databases. As an Intel Software Innovator and Intel Black Belt Software Developer he is strongly involved in the development of open-source software for gesture control with 3D-cameras like e.g. Intel RealSense and has built an Augmented Reality wearable prototype device with his team based on this technology. Furthermore, he gives many talks on national and international conferences about Internet of Things, 3D-camera technologies, Augmented Reality and Test Driven Development as well. He was awarded with the JavaOne Rockstar award and is an author for the technical blog ParrotsOnJava.com.\n\nThomas Reifenberger works as an IT Consultant for the Munich based consulting company TNG Technology Consulting GmbH. Before joining the company in 2012 studied physics at the Technical University of Munich. Since then he worked for various customers in different sectors – with his technical focus areas being Java, Perl, Groovy & Python. Besides that he is involved in the company’s hardware hacking team, where he is mainly working with IoT projects. Besides other tasks, he is involved in deployment automation, Linux administration and networking for the team.\n\nAbstract\n\nTags: vr telepresence gesture control 3d cameras python robotics hardware iot\n\nUsing humanoid robots, VR glasses and 3D cameras you can experience the world through the eyes of a robot and control it via gestures. We built a telepresence robotics system based on a Nao robot, an Oculus Rift and a Kinect One to realize an immersive \"out-of-body experience\" as in \"Avatar\".\nDescription\n\nUsing humanoid robots, virtual reality glasses and 3D camera sensors you can experience the world through the eyes of a robot and control it via gestures. The hardware hacking team of TNG Technology Consulting has built a telepresence robotics system based on a Nao robot, an Oculus Rift and a Kinect One. Using these components you can realize an immersive \"out-of-body experience\" - similar to that of the film \"Avatar\".\n\nThis talk shows how easy it is to program the robot using Python. We will do this by some live coding examples. Following a live demonstration of the showcase we will guide you through the complete program flow of the telepresence system. The speakers will share some insights about the challenges they faced during its implementation. The history of telepresence robotics, current trends and examples for real world fields of application will also be focused.",
        "thumbnail": "https://i.ytimg.com/vi_webp/SGpdc-9QAWE/maxresdefault.webp",
        "age_limit": 0,
        "is_live": false,
        "was_live": false,
        "availability": "public",
        "webpage_url": "https://www.youtube.com/watch?v=SGpdc-9QAWE",
        "original_url": "https://www.youtube.com/watch?v=SGpdc-9QAWE"
    }
]